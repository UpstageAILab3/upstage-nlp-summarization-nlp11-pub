{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델과 토크나이저 로드\n",
    "model_name = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트와 인스트럭션 정의\n",
    "PROMPT = '''당신은 고성능 한국어 대화 요약 AI입니다. 주어진 대화를 읽고 아래의 지침에 따라 핵심 내용을 간결하게 요약해서 요약문만 반환해주세요.'''\n",
    "instruction = '''\n",
    "1. 대화에서 가장 중요한 정보를 파악하고 전달하세요.\n",
    "2. 요약은 원본 대화 길이의 20% 이내로 간략하게 작성하세요.\n",
    "3. 대화 내에서 언급된 중요한 명명된 개체(예: 사람 이름, 기업명 등)를 보존하세요.\n",
    "4. 관찰자의 관점에서 요약을 작성하되, 화자들의 의도를 이해하고 반영하세요.\n",
    "5. 은어나 약어를 사용하지 말고, 공식적으로 사용되는 언어로 작성하세요.\n",
    "6. 대화의 전체적인 맥락과 주요 주제를 파악하세요.\n",
    "7. 중요한 정보와 결론을 추출하여 논리적으로 구성하세요.\n",
    "---\n",
    "#Person1#: 안녕하세요, 오늘 하루 어떠셨어요? \n",
    "#Person2#: 요즘 숨쉬기가 좀 힘들어요.\n",
    "#Person1#: 최근에 감기 같은 것에 걸리신 적이 있나요?\n",
    "#Person2#: 아니요, 감기는 아니에요. 그냥 숨을 쉴 때마다 가슴이 무겁게 느껴져요.\n",
    "#Person1#: 알고 있는 알레르기가 있나요?\n",
    "#Person2#: 아니요, 알고 있는 알레르기는 없어요.\n",
    "#Person1#: 이런 증상이 항상 나타나나요, 아니면 활동할 때 주로 나타나나요?\n",
    "#Person2#: 운동을 할 때 많이 나타나요.\n",
    "#Person1#: 저는 당신을 폐 전문의에게 보내서 천식에 대한 검사를 받게 할 거예요.\n",
    "#Person2#: 도와주셔서 감사합니다, 의사 선생님.\n",
    "'''\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{instruction}\"}\n",
    "    ]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    # max_new_tokens=400,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=False,\n",
    "    repetition_penalty = 1.1\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
