{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Development Environment\n",
    "\n",
    "Our first step is to install the Hugging Face Libraries, including transformers and datasets. Running the following cell will install all the required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "GPU device name: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "    print(f\"GPU device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU is not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: pytesseract in /opt/conda/lib/python3.10/site-packages (0.3.13)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.28.0)\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/75/35/07c9879163b603f0e464b0f6e6e628a2340cfc7cdc5ca8e7d52d776710d4/transformers-4.44.2-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\n",
      "Requirement already satisfied: rouge-score in /opt/conda/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.17.1)\n",
      "Requirement already satisfied: py7zr in /opt/conda/lib/python3.10/site-packages (0.22.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from pytesseract) (23.1)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from pytesseract) (9.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.20,>=0.19 from https://files.pythonhosted.org/packages/40/4f/eb78de4af3b17b589f43a369cbf0c3a7173f25c3d2cd93068852c07689aa/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# python\n",
    "!pip install pytesseract transformers datasets rouge-score nltk tensorboard py7zr --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install git-fls for pushing model and logs to the hugging face hub\n",
    "!sudo apt-get install git-lfs --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will use the [Hugging Face Hub](https://huggingface.co/models) as a remote model versioning service. To be able to push our model to the Hub, you need to register on the [Hugging Face](https://huggingface.co/join). \n",
    "If you already have an account, you can skip this step. \n",
    "After you have an account, we will use the `notebook_login` util from the `huggingface_hub` package to log into our account and store our token (access key) on the disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e88856538d427e8733a5f4f81d539b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "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\n",
    "T5_DialogueSum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare dialogueSum dataset from local\n",
    "- This DialogueSum dataset was originally in English but was translated into Korean by teachers using the Solar API for educational purposes. However, the translation seemed somewhat unnatural for native Korean speakers, so I used the Solar API to retranslate it into English to facilitate a more accurate summarization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the `dialogueSum` dataset, we use the `load_dataset()` method from the ü§ó Datasets library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"dialoguSum_Solar_koen\"\n",
    "# huggingface hub model id\n",
    "model_id=\"beomi/gemma-ko-2b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 12457\n",
      "val dataset size: 499\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset('csv', data_files={'train': \"/data/ephemeral/home/data/train.csv\", 'val': \"/data/ephemeral/home/data/dev.csv\"})\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"val dataset size: {len(dataset['val'])}\")\n",
    "\n",
    "# Train dataset size: 12457\n",
    "# Test dataset size: 499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['fname', 'dialogue', 'summary', 'topic'],\n",
       "    num_rows: 12457\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets checkout an example of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialogue: \n",
      "#Person1#: Ïò§Îäò ÏïÑÏπ®Ïóê ÏöîÏ≤≠ÌïòÏã† Î¨∏ÏÑúÏûÖÎãàÎã§.\n",
      "#Person2#: Ïò§, Ï†ïÎßê Ìö®Ïú®Ï†ÅÏù¥ÏãúÎÑ§Ïöî. ÎÇ¥Ïùº Ï§Ñ Ï§Ñ ÏïåÏïòÎäîÎç∞ Í∞êÏÇ¨Ìï©ÎãàÎã§.\n",
      "#Person1#: Ï≤úÎßåÏóêÏöî. ÏöîÏ¶ò Ï†úÍ∞Ä Í∞ÄÏû• ÌÅ∞ ÏïΩÏ†êÏù∏ ÎØ∏Î£®Í∏∞Ïóê ÎåÄÌï¥ ÎßéÏù¥ ÏÉùÍ∞ÅÌïòÍ≥† ÏûàÏñ¥Ïöî. ÏÉùÍ∞ÅÌï†ÏàòÎ°ù Ï†úÍ∞Ä ÎÑàÎ¨¥ Î¨¥ÏßàÏÑúÌïú Í≤É Í∞ôÏïÑÏÑú Ïä§Ïä§Î°úÎ•º ÎØ∏ÏõåÌïòÍ≤å ÎêòÎÑ§Ïöî. Í∞ÄÎä•Ìïú Ìïú Îπ®Î¶¨ ÏÉÅÌô©ÏùÑ Î∞îÍæ∏Î†§Í≥† Í≤∞Ïã¨ÌñàÏñ¥Ïöî. Í∑∏Î†áÏßÄ ÏïäÏúºÎ©¥ ÎÇòÏ§ëÏóê Îçî ÌõÑÌöåÌï† Í≤É Í∞ôÏïÑÏöî.\n",
      "#Person2#: ÌûòÎÇ¥Îäî ÏÜåÏãùÏù¥ÎÑ§Ïöî, ÏûòÌïòÍ≥† Í≥ÑÏãúÎÑ§Ïöî! Í∑∏Îüº Ïñ¥Îñ§ Ìï¥Í≤∞Ï±ÖÏùÑ ÏÉùÍ∞ÅÌïòÏÖ®ÎÇòÏöî?\n",
      "#Person1#: Í∞ÄÏû• Ïú†Ïö©Ìïú Î∞©Î≤ïÏùÄ Í≥ÑÌöçÏùÑ ÏÑ∏Ïö∞Í≥† Ïö∞ÏÑ†ÏàúÏúÑÎ•º Ï†ïÌïòÎäî Í≤ÉÏûÖÎãàÎã§. Ïù¥Í≤ÉÏù¥ ÏãúÍ∞ÑÏùÑ Ïûò Í¥ÄÎ¶¨ÌïòÍ≥† Í∞ÄÏû• Ï§ëÏöîÌïú ÏùºÏùÑ Î®ºÏ†Ä Ï≤òÎ¶¨ÌïòÎäîÎç∞ ÎèÑÏõÄÏù¥ Îê©ÎãàÎã§.\n",
      "#Person2#: ÎÇòÏÅòÏßÄ ÏïäÎÑ§Ïöî! Îçî ÎÇòÏùÄ ÏÑ±Í≥ºÎäî Îã®ÏßÄ Îçî ÎßéÏùÄ ÏùºÏùÑ ÌïòÎäî Í≤ÉÏù¥ ÏïÑÎãàÎùº, Ïò¨Î∞îÎ•∏ ÏùºÏóê ÏßëÏ§ëÌïòÎäî Í≤ÉÏûÖÎãàÎã§.\n",
      "---------------\n",
      "summary: \n",
      "#Person1#ÏùÄ ÎØ∏Î£®Í∏∞Î•º ÏóÜÏï†Í≥†Ïûê ÌïòÎ©∞ Í≥ÑÌöçÏùÑ ÏÑ∏Ïö∞Í≥† Ïö∞ÏÑ†ÏàúÏúÑÎ•º Ï†ïÌïòÍ∏∞Î°ú Í≤∞Ï†ïÌñàÎã§. #Person2#Îäî Ïù¥Í≤ÉÏù¥ #Person1#ÏóêÍ≤å Ï¢ãÎã§Í≥† ÏÉùÍ∞ÅÌïúÎã§.\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "from random import randrange        \n",
    "\n",
    "\n",
    "sample = dataset['train'][randrange(len(dataset[\"train\"]))]\n",
    "print(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n",
    "print(f\"summary: \\n{sample['summary']}\\n---------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model we need to convert our inputs (text) to token IDs. This is done by a ü§ó Transformers Tokenizer. If you are not sure what this means check out [chapter 6](https://huggingface.co/course/chapter6/1?fw=tf) of the Hugging Face Course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SentencePiece\n",
      "  Obtaining dependency information for SentencePiece from https://files.pythonhosted.org/packages/a6/27/33019685023221ca8ed98e8ceb7ae5e166032686fa3662c68f1f1edf334e/sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: SentencePiece\n",
      "Successfully installed SentencePiece-0.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# Load tokenizer of FLAN-t5-base\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_id) # ÏóêÎü¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE:\n",
      "#Person1#: ÏïàÎÖïÌïòÏÑ∏Ïöî, Ïä§ÎØ∏Ïä§Ïî®. Ï†ÄÎäî Ìò∏ÌÇ®Ïä§ ÏùòÏÇ¨ÏûÖÎãàÎã§. Ïò§Îäò Ïôú Ïò§ÏÖ®ÎÇòÏöî?\n",
      "#Person2#: Í±¥Í∞ïÍ≤ÄÏßÑÏùÑ Î∞õÎäî Í≤ÉÏù¥ Ï¢ãÏùÑ Í≤É Í∞ôÏïÑÏÑúÏöî.\n",
      "#Person1#: Í∑∏Î†áÍµ∞Ïöî, ÎãπÏã†ÏùÄ 5ÎÖÑ ÎèôÏïà Í±¥Í∞ïÍ≤ÄÏßÑÏùÑ Î∞õÏßÄ ÏïäÏïòÏäµÎãàÎã§. Îß§ÎÖÑ Î∞õÏïÑÏïº Ìï©ÎãàÎã§.\n",
      "#Person2#: ÏïåÍ≥† ÏûàÏäµÎãàÎã§. ÌïòÏßÄÎßå ÏïÑÎ¨¥ Î¨∏Ï†úÍ∞Ä ÏóÜÎã§Î©¥ Ïôú ÏùòÏÇ¨Î•º ÎßåÎÇòÎü¨ Í∞ÄÏïº ÌïòÎÇòÏöî?\n",
      "#Person1#: Ïã¨Í∞ÅÌïú ÏßàÎ≥ëÏùÑ ÌîºÌïòÎäî Í∞ÄÏû• Ï¢ãÏùÄ Î∞©Î≤ïÏùÄ Ïù¥Î•º Ï°∞Í∏∞Ïóê Î∞úÍ≤¨ÌïòÎäî Í≤ÉÏûÖÎãàÎã§. Í∑∏Îü¨Îãà ÎãπÏã†Ïùò Í±¥Í∞ïÏùÑ ÏúÑÌï¥ ÏµúÏÜåÌïú Îß§ÎÖÑ Ìïú Î≤àÏùÄ Ïò§ÏÑ∏Ïöî.\n",
      "#Person2#: ÏïåÍ≤†ÏäµÎãàÎã§.\n",
      "#Person1#: Ïó¨Í∏∞ Î≥¥ÏÑ∏Ïöî. ÎãπÏã†Ïùò ÎààÍ≥º Í∑ÄÎäî Í¥úÏ∞ÆÏïÑ Î≥¥ÏûÖÎãàÎã§. ÍπäÍ≤å Ïà®ÏùÑ Îì§Ïù¥Ïâ¨ÏÑ∏Ïöî. Ïä§ÎØ∏Ïä§Ïî®, Îã¥Î∞∞ ÌîºÏö∞ÏãúÎÇòÏöî?\n",
      "#Person2#: ÎÑ§.\n",
      "#Person1#: ÎãπÏã†ÎèÑ ÏïåÎã§ÏãúÌîº, Îã¥Î∞∞Îäî ÌèêÏïîÍ≥º Ïã¨Ïû•Î≥ëÏùò Ï£ºÏöî ÏõêÏù∏ÏûÖÎãàÎã§. Ï†ïÎßêÎ°ú ÎÅäÏúºÏÖîÏïº Ìï©ÎãàÎã§. \n",
      "#Person2#: ÏàòÎ∞± Î≤à ÏãúÎèÑÌñàÏßÄÎßå, ÏäµÍ¥ÄÏùÑ Î≤ÑÎ¶¨Îäî Í≤ÉÏù¥ Ïñ¥Î†µÏäµÎãàÎã§.\n",
      "#Person1#: Ïö∞Î¶¨Îäî ÎèÑÏõÄÏù¥ Îê† Ïàò ÏûàÎäî ÏàòÏóÖÍ≥º ÏïΩÎ¨ºÎì§ÏùÑ Ï†úÍ≥µÌïòÍ≥† ÏûàÏäµÎãàÎã§. ÎÇòÍ∞ÄÍ∏∞ Ï†ÑÏóê Îçî ÎßéÏùÄ Ï†ïÎ≥¥Î•º ÎìúÎ¶¨Í≤†ÏäµÎãàÎã§.\n",
      "#Person2#: ÏïåÍ≤†ÏäµÎãàÎã§, Í∞êÏÇ¨Ìï©ÎãàÎã§, ÏùòÏÇ¨ÏÑ†ÏÉùÎãò.\n",
      "\n",
      "ENCODED SENTENCE:\n",
      "[250115, 259, 267, 7269, 128550, 56527, 261, 14119, 4532, 2441, 17519, 260, 8344, 988, 34274, 65059, 2441, 259, 91758, 5068, 260, 259, 38933, 259, 50272, 4558, 98793, 65791, 291, 259, 250111, 259, 267, 58736, 51935, 4141, 611, 15748, 988, 14345, 22881, 611, 3364, 18084, 2652, 1947, 4661, 260, 259, 250115, 259, 267, 1884, 18978, 126079, 261, 10869, 3158, 869, 430, 2479, 259, 25045, 58736, 51935, 4141, 611, 15748, 1426, 4354, 103060, 260, 10100, 2479, 70556, 5758, 259, 5699, 260, 259, 250111, 259, 267, 10019, 1000, 6799, 260, 259, 21634, 3759, 4498, 18697, 929, 10174, 52068, 259, 50272, 259, 649, 49294, 12261, 2924, 3815, 4021, 5758, 15481, 4661, 291, 259, 250115, 259, 267, 28463, 14169, 839, 40990, 21196, 611, 20488, 1953, 4021, 1870, 259, 27728, 22113, 869, 259, 36709, 7830, 29224, 11792, 17284, 1953, 3364, 5068, 260, 259, 19393, 6165, 10869, 3158, 649, 58736, 611, 259, 11196, 259, 78730, 839, 10100, 2479, 259, 839, 17273, 869, 4558, 70134, 260, 259, 250111, 259, 267, 10019, 95226, 260, 259, 250115, 259, 267, 5741, 1622, 3967, 70134, 260, 10869, 3158, 649, 31138, 1603, 49757, 988, 259, 148933, 135036, 2652, 3967, 5068, 260, 259, 61123, 2317, 259, 75242, 611, 259, 7222, 38896, 70134, 260, 14119, 4532, 2441, 17519, 261, 33400, 14941, 20488, 3216, 1601, 65791, 291, 259, 250111, 259, 267, 33184, 260, 259, 250115, 259, 267, 10869, 3158, 1247, 10019, 1654, 1601, 12341, 261, 33400, 14941, 988, 86480, 52606, 1603, 28463, 1870, 21196, 649, 3919, 4661, 10051, 1812, 5068, 260, 6104, 13958, 1235, 259, 139790, 20282, 33744, 5758, 259, 5699, 260, 259, 250111, 259, 267, 1566, 20029, 17273, 6463, 1247, 98717, 261, 259, 18198, 7882, 611, 46351, 26830, 14345, 5484, 83449, 10175, 260, 259, 250115, 259, 267, 9166, 988, 259, 1247, 13769, 747, 259, 14600, 1566, 4214, 1566, 9443, 1603, 24700, 6638, 17597, 17054, 2551, 6799, 260, 259, 82881, 1622, 259, 45960, 6821, 7552, 869, 16175, 1023, 259, 84336, 95226, 260, 259, 250111, 259, 267, 10019, 95226, 261, 41415, 5699, 261, 259, 91758, 4493, 8221, 11868, 260, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "DECODED SENTENCE:\n",
      "#Person1# : ÏïàÎÖïÌïòÏÑ∏Ïöî, Ïä§ÎØ∏Ïä§Ïî®. Ï†ÄÎäî Ìò∏ÌÇ®Ïä§ ÏùòÏÇ¨ÏûÖÎãàÎã§. Ïò§Îäò Ïôú Ïò§ÏÖ®ÎÇòÏöî? #Person2# : Í±¥Í∞ïÍ≤ÄÏßÑÏùÑ Î∞õÎäî Í≤ÉÏù¥ Ï¢ãÏùÑ Í≤É Í∞ôÏïÑÏÑúÏöî. #Person1# : Í∑∏Î†áÍµ∞Ïöî, ÎãπÏã†ÏùÄ 5ÎÖÑ ÎèôÏïà Í±¥Í∞ïÍ≤ÄÏßÑÏùÑ Î∞õÏßÄ ÏïäÏïòÏäµÎãàÎã§. Îß§ÎÖÑ Î∞õÏïÑÏïº Ìï©ÎãàÎã§. #Person2# : ÏïåÍ≥† ÏûàÏäµÎãàÎã§. ÌïòÏßÄÎßå ÏïÑÎ¨¥ Î¨∏Ï†úÍ∞Ä ÏóÜÎã§Î©¥ Ïôú ÏùòÏÇ¨Î•º ÎßåÎÇòÎü¨ Í∞ÄÏïº ÌïòÎÇòÏöî? #Person1# : Ïã¨Í∞ÅÌïú ÏßàÎ≥ëÏùÑ ÌîºÌïòÎäî Í∞ÄÏû• Ï¢ãÏùÄ Î∞©Î≤ïÏùÄ Ïù¥Î•º Ï°∞Í∏∞Ïóê Î∞úÍ≤¨ÌïòÎäî Í≤ÉÏûÖÎãàÎã§. Í∑∏Îü¨Îãà ÎãπÏã†Ïùò Í±¥Í∞ïÏùÑ ÏúÑÌï¥ ÏµúÏÜåÌïú Îß§ÎÖÑ Ìïú Î≤àÏùÄ Ïò§ÏÑ∏Ïöî. #Person2# : ÏïåÍ≤†ÏäµÎãàÎã§. #Person1# : Ïó¨Í∏∞ Î≥¥ÏÑ∏Ïöî. ÎãπÏã†Ïùò ÎààÍ≥º Í∑ÄÎäî Í¥úÏ∞ÆÏïÑ Î≥¥ÏûÖÎãàÎã§. ÍπäÍ≤å Ïà®ÏùÑ Îì§Ïù¥Ïâ¨ÏÑ∏Ïöî. Ïä§ÎØ∏Ïä§Ïî®, Îã¥Î∞∞ ÌîºÏö∞ÏãúÎÇòÏöî? #Person2# : ÎÑ§. #Person1# : ÎãπÏã†ÎèÑ ÏïåÎã§ÏãúÌîº, Îã¥Î∞∞Îäî ÌèêÏïîÍ≥º Ïã¨Ïû•Î≥ëÏùò Ï£ºÏöî ÏõêÏù∏ÏûÖÎãàÎã§. Ï†ïÎßêÎ°ú ÎÅäÏúºÏÖîÏïº Ìï©ÎãàÎã§. #Person2# : ÏàòÎ∞± Î≤à ÏãúÎèÑÌñàÏßÄÎßå, ÏäµÍ¥ÄÏùÑ Î≤ÑÎ¶¨Îäî Í≤ÉÏù¥ Ïñ¥Î†µÏäµÎãàÎã§. #Person1# : Ïö∞Î¶¨Îäî ÎèÑÏõÄÏù¥ Îê† Ïàò ÏûàÎäî ÏàòÏóÖÍ≥º ÏïΩÎ¨ºÎì§ÏùÑ Ï†úÍ≥µÌïòÍ≥† ÏûàÏäµÎãàÎã§. ÎÇòÍ∞ÄÍ∏∞ Ï†ÑÏóê Îçî ÎßéÏùÄ Ï†ïÎ≥¥Î•º ÎìúÎ¶¨Í≤†ÏäµÎãàÎã§. #Person2# : ÏïåÍ≤†ÏäµÎãàÎã§, Í∞êÏÇ¨Ìï©ÎãàÎã§, ÏùòÏÇ¨ÏÑ†ÏÉùÎãò.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "# ÏûëÎèô Ïûò ÎêòÎäîÏßÄ ÌôïÏù∏\n",
    "# Define a test sentence\n",
    "sentence = dataset[\"train\"]['dialogue'][0]\n",
    "\n",
    "\n",
    "# Encode the sentence using the tokenizer, returning PyTorch tensors\n",
    "sentence_encoded = tokenizer(sentence, \n",
    "                             max_length=max_source_length, \n",
    "                             padding=\"max_length\", \n",
    "                             truncation=True, \n",
    "                             add_special_tokens=True)\n",
    "\n",
    "# Decode the encoded sentence, skipping special tokens\n",
    "sentence_decoded = tokenizer.decode(\n",
    "        sentence_encoded[\"input_ids\"], \n",
    "        max_length=max_target_length, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        add_special_tokens=True,\n",
    "        skip_special_tokens=False\n",
    "    )\n",
    "\n",
    "# Print SENTENCE\n",
    "print('SENTENCE:')\n",
    "print(sentence)\n",
    "\n",
    "# Print the encoded sentence's representation\n",
    "print('\\nENCODED SENTENCE:')\n",
    "print(sentence_encoded[\"input_ids\"])\n",
    "\n",
    "# Print the decoded sentence\n",
    "print('\\nDECODED SENTENCE:')\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before we can start training we need to preprocess our data. Abstractive Summarization is a text2text-generation task. This means our model will take a text as input and generate a summary as output. For this we want to understand how long our input and output will be to be able to efficiently batch our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4999e2a7a94e8e974b4ec4fed06fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12956 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 1024\n",
      "Min source length: 155\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7d9bdcc9d14d1b8b90111604da4a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12956 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 888\n",
      "Min target length: 23\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# The maximum total input sequence length after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"val\"]]).map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "min_source_length = min([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Min source length: {min_source_length}\")\n",
    "\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"val\"]]).map(lambda x: tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "print(f\"Max target length: {max_target_length}\")\n",
    "min_target_length = min([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "print(f\"Min target length: {min_target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'#CarNumber#' is not in the vocabulary.\n",
      "'#SSN#' is not in the vocabulary.\n",
      "'#PhoneNumber#' is not in the vocabulary.\n",
      "'#PassportNumber#' is not in the vocabulary.\n",
      "'#Email#' is not in the vocabulary.\n",
      "'#CardNumber#' is not in the vocabulary.\n",
      "'#Address#' is not in the vocabulary.\n",
      "'#DateOfBirth#' is not in the vocabulary.\n",
      "'#Person4#' is not in the vocabulary.\n",
      "'#Person7#' is not in the vocabulary.\n",
      "'#Person3#' is not in the vocabulary.\n",
      "'#Person2#' is not in the vocabulary.\n",
      "'#Person#' is not in the vocabulary.\n",
      "'#Person6#' is not in the vocabulary.\n",
      "'#Person5#' is not in the vocabulary.\n",
      "'#Person1#' is not in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "special_tokens = ['#CarNumber#', '#SSN#', '#PhoneNumber#', '#PassportNumber#', '#Email#', '#CardNumber#', '#Address#', '#DateOfBirth#', \\\n",
    "'#Person4#', '#Person7#', '#Person3#', '#Person2#', '#Person#', '#Person6#', '#Person5#', '#Person1#']\n",
    "for token in special_tokens:\n",
    "    if token in tokenizer.get_vocab():\n",
    "        print(f\"'{token}' is already in the vocabulary.\")\n",
    "    else:\n",
    "        print(f\"'{token}' is not in the vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab size: 50257\n",
      "New vocab size: 50273\n"
     ]
    }
   ],
   "source": [
    "original_vocab_size = len(tokenizer)\n",
    "\n",
    "special_tokens = ['#CarNumber#', '#SSN#', '#PhoneNumber#', '#PassportNumber#', '#Email#', '#CardNumber#', '#Address#', '#DateOfBirth#', \\\n",
    "'#Person4#', '#Person7#', '#Person3#', '#Person2#', '#Person#', '#Person6#', '#Person5#', '#Person1#']\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
    "new_vocab_size = len(tokenizer)\n",
    "\n",
    "print(f\"Original vocab size: {original_vocab_size}\")\n",
    "print(f\"New vocab size: {new_vocab_size}\")\n",
    "\n",
    "# Original vocab size: 32100\n",
    "# New vocab size: 32116"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## token add: ÌïúÍµ≠Ïñ¥Ïùò Í≤ΩÏö∞ ÌïÑÏàò: ÏùºÎã® ÏïàÌï¥Î¥Ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab size: 32116\n",
      "New vocab size: 182717\n"
     ]
    }
   ],
   "source": [
    "# token Ï∂îÍ∞Ä\n",
    "import pandas as pd\n",
    "\n",
    "Original_vocab_size = len(tokenizer)\n",
    "print(f\"Original vocab size: {Original_vocab_size}\")\n",
    "# Step 1: Extract unique words from the dataset\n",
    "unique_words1 = set()\n",
    "for sentence in dataset['train']['dialogue']:\n",
    "    words = sentence.split()  # Simple split, you might want to use a tokenizer for better results\n",
    "    unique_words1.update(words)\n",
    "unique_words2 = set()\n",
    "for sentence in dataset['train']['summary']:\n",
    "    words = sentence.split()  # Simple split, you might want to use a tokenizer for better results\n",
    "    unique_words2.update(words)\n",
    "\n",
    "unique_words3 = set()\n",
    "for sentence in dataset['val']['dialogue']:\n",
    "    words = sentence.split()  # Simple split, you might want to use a tokenizer for better results\n",
    "    unique_words3.update(words)\n",
    "unique_words4 = set()\n",
    "for sentence in dataset['val']['summary']:\n",
    "    words = sentence.split()  # Simple split, you might want to use a tokenizer for better results\n",
    "    unique_words4.update(words)\n",
    "\n",
    "test = pd.read_csv(r'/data/ephemeral/home/data/test_en.csv')\n",
    "unique_words5 = set()\n",
    "for sentence in test['dialogue_en']:\n",
    "    words = sentence.split()  # Simple split, you might want to use a tokenizer for better results\n",
    "    unique_words5.update(words)    \n",
    "    \n",
    "# Step 2: Add these words to the tokenizer vocabulary\n",
    "# The tokenizer will automatically handle the splitting and add only those not already in the vocab\n",
    "tokenizer.add_tokens(list(unique_words1))\n",
    "tokenizer.add_tokens(list(unique_words2))\n",
    "tokenizer.add_tokens(list(unique_words3))\n",
    "tokenizer.add_tokens(list(unique_words4))\n",
    "tokenizer.add_tokens(list(unique_words5))\n",
    "tokenizer.add_tokens(list(special_tokens))\n",
    "# Step 3: Check the new vocabulary size\n",
    "new_vocab_size = len(tokenizer)\n",
    "print(f\"New vocab size: {new_vocab_size}\")\n",
    "\n",
    "# Original vocab size: 32116\n",
    "# New vocab size: 182717"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizer ÌôïÏù∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE:\n",
      "#Person1#: ÏïàÎÖïÌïòÏÑ∏Ïöî, Ïä§ÎØ∏Ïä§Ïî®. Ï†ÄÎäî Ìò∏ÌÇ®Ïä§ ÏùòÏÇ¨ÏûÖÎãàÎã§. Ïò§Îäò Ïôú Ïò§ÏÖ®ÎÇòÏöî?\n",
      "#Person2#: Í±¥Í∞ïÍ≤ÄÏßÑÏùÑ Î∞õÎäî Í≤ÉÏù¥ Ï¢ãÏùÑ Í≤É Í∞ôÏïÑÏÑúÏöî.\n",
      "#Person1#: Í∑∏Î†áÍµ∞Ïöî, ÎãπÏã†ÏùÄ 5ÎÖÑ ÎèôÏïà Í±¥Í∞ïÍ≤ÄÏßÑÏùÑ Î∞õÏßÄ ÏïäÏïòÏäµÎãàÎã§. Îß§ÎÖÑ Î∞õÏïÑÏïº Ìï©ÎãàÎã§.\n",
      "#Person2#: ÏïåÍ≥† ÏûàÏäµÎãàÎã§. ÌïòÏßÄÎßå ÏïÑÎ¨¥ Î¨∏Ï†úÍ∞Ä ÏóÜÎã§Î©¥ Ïôú ÏùòÏÇ¨Î•º ÎßåÎÇòÎü¨ Í∞ÄÏïº ÌïòÎÇòÏöî?\n",
      "#Person1#: Ïã¨Í∞ÅÌïú ÏßàÎ≥ëÏùÑ ÌîºÌïòÎäî Í∞ÄÏû• Ï¢ãÏùÄ Î∞©Î≤ïÏùÄ Ïù¥Î•º Ï°∞Í∏∞Ïóê Î∞úÍ≤¨ÌïòÎäî Í≤ÉÏûÖÎãàÎã§. Í∑∏Îü¨Îãà ÎãπÏã†Ïùò Í±¥Í∞ïÏùÑ ÏúÑÌï¥ ÏµúÏÜåÌïú Îß§ÎÖÑ Ìïú Î≤àÏùÄ Ïò§ÏÑ∏Ïöî.\n",
      "#Person2#: ÏïåÍ≤†ÏäµÎãàÎã§.\n",
      "#Person1#: Ïó¨Í∏∞ Î≥¥ÏÑ∏Ïöî. ÎãπÏã†Ïùò ÎààÍ≥º Í∑ÄÎäî Í¥úÏ∞ÆÏïÑ Î≥¥ÏûÖÎãàÎã§. ÍπäÍ≤å Ïà®ÏùÑ Îì§Ïù¥Ïâ¨ÏÑ∏Ïöî. Ïä§ÎØ∏Ïä§Ïî®, Îã¥Î∞∞ ÌîºÏö∞ÏãúÎÇòÏöî?\n",
      "#Person2#: ÎÑ§.\n",
      "#Person1#: ÎãπÏã†ÎèÑ ÏïåÎã§ÏãúÌîº, Îã¥Î∞∞Îäî ÌèêÏïîÍ≥º Ïã¨Ïû•Î≥ëÏùò Ï£ºÏöî ÏõêÏù∏ÏûÖÎãàÎã§. Ï†ïÎßêÎ°ú ÎÅäÏúºÏÖîÏïº Ìï©ÎãàÎã§. \n",
      "#Person2#: ÏàòÎ∞± Î≤à ÏãúÎèÑÌñàÏßÄÎßå, ÏäµÍ¥ÄÏùÑ Î≤ÑÎ¶¨Îäî Í≤ÉÏù¥ Ïñ¥Î†µÏäµÎãàÎã§.\n",
      "#Person1#: Ïö∞Î¶¨Îäî ÎèÑÏõÄÏù¥ Îê† Ïàò ÏûàÎäî ÏàòÏóÖÍ≥º ÏïΩÎ¨ºÎì§ÏùÑ Ï†úÍ≥µÌïòÍ≥† ÏûàÏäµÎãàÎã§. ÎÇòÍ∞ÄÍ∏∞ Ï†ÑÏóê Îçî ÎßéÏùÄ Ï†ïÎ≥¥Î•º ÎìúÎ¶¨Í≤†ÏäµÎãàÎã§.\n",
      "#Person2#: ÏïåÍ≤†ÏäµÎãàÎã§, Í∞êÏÇ¨Ìï©ÎãàÎã§, ÏùòÏÇ¨ÏÑ†ÏÉùÎãò.\n",
      "\n",
      "ENCODED SENTENCE:\n",
      "[50272, 25, 23821, 243, 230, 167, 227, 243, 47991, 246, 168, 226, 116, 168, 248, 242, 11, 23821, 232, 97, 167, 107, 116, 168, 232, 97, 168, 242, 101, 13, 23821, 254, 222, 167, 232, 242, 220, 169, 246, 116, 169, 224, 101, 168, 232, 97, 23821, 251, 246, 168, 8955, 168, 252, 227, 46695, 230, 46695, 97, 13, 23821, 246, 97, 167, 232, 246, 23821, 247, 250, 23821, 246, 97, 168, 227, 101, 167, 224, 246, 168, 248, 242, 30, 198, 50268, 25, 220, 166, 109, 112, 166, 108, 243, 166, 110, 222, 168, 100, 226, 35975, 226, 31619, 108, 249, 167, 232, 242, 220, 166, 110, 225, 35975, 112, 23821, 95, 233, 35975, 226, 220, 166, 110, 225, 220, 166, 108, 247, 168, 243, 226, 168, 226, 250, 168, 248, 242, 13, 198, 50272, 25, 220, 166, 115, 116, 167, 254, 229, 166, 113, 108, 168, 248, 242, 11, 31619, 233, 117, 168, 233, 254, 35975, 222, 642, 167, 227, 226, 31619, 237, 247, 168, 243, 230, 220, 166, 109, 112, 166, 108, 243, 166, 110, 222, 168, 100, 226, 35975, 226, 31619, 108, 249, 168, 100, 222, 23821, 243, 232, 168, 243, 246, 168, 232, 113, 46695, 230, 46695, 97, 13, 31619, 100, 97, 167, 227, 226, 31619, 108, 249, 168, 243, 226, 168, 243, 120, 220, 47991, 102, 46695, 230, 46695, 97, 13, 198, 50268, 25, 23821, 243, 234, 166, 111, 254, 23821, 252, 230, 168, 232, 113, 46695, 230, 46695, 97, 13, 220, 47991, 246, 168, 100, 222, 167, 100, 234, 23821, 243, 226, 167, 105, 112, 31619, 105, 116, 168, 254, 250, 166, 108, 222, 23821, 245, 228, 46695, 97, 167, 102, 112, 23821, 247, 250, 23821, 251, 246, 168, 8955, 167, 98, 120, 31619, 100, 234, 167, 224, 246, 167, 253, 105, 220, 166, 108, 222, 168, 243, 120, 220, 47991, 246, 167, 224, 246, 168, 248, 242, 30, 198, 50272, 25, 23821, 233, 105, 166, 108, 223, 47991, 250, 23821, 100, 230, 167, 111, 239, 35975, 226, 220, 169, 242, 120, 47991, 246, 167, 232, 242, 220, 166, 108, 222, 168, 252, 98, 23821, 95, 233, 35975, 222, 31619, 108, 102, 167, 110, 243, 35975, 222, 23821, 251, 112, 167, 98, 120, 23821, 94, 108, 166, 116, 108, 168, 245, 238, 31619, 108, 250, 166, 110, 105, 47991, 246, 167, 232, 242, 220, 166, 110, 225, 168, 252, 227, 46695, 230, 46695, 97, 13, 220, 166, 115, 116, 167, 253, 105, 46695, 230, 31619, 233, 117, 168, 233, 254, 35975, 246, 220, 166, 109, 112, 166, 108, 243, 35975, 226, 23821, 250, 226, 47991, 112, 23821, 113, 250, 168, 228, 234, 47991, 250, 31619, 100, 97, 167, 227, 226, 220, 47991, 250, 31619, 110, 230, 35975, 222, 23821, 246, 97, 168, 226, 116, 168, 248, 242, 13, 198, 50268, 25, 23821, 243, 234, 166, 110, 254, 168, 232, 113, 46695, 230, 46695, 97, 13, 198, 50272, 25, 23821, 245, 105, 166, 116, 108, 31619, 111, 112, 168, 226, 116, 168, 248, 242, 13, 31619, 233, 117, 168, 233, 254, 35975, 246, 31619, 230, 230, 166, 111, 120, 220, 166, 115, 222, 167, 232, 242, 220, 166, 112, 250, 168, 108, 106, 168, 243, 226, 31619, 111, 112, 168, 252, 227, 46695, 230, 46695, 97, 13, 220, 166, 117, 232, 166, 110, 234, 23821, 230, 101, 35975, 226, 31619, 241, 97, 35975, 112, 168, 231, 105, 168, 226, 116, 168, 248, 242, 13, 23821, 232, 97, 167, 107, 116, 168, 232, 97, 168, 242, 101, 11, 31619, 233, 112, 167, 108, 108, 220, 169, 242, 120, 168, 248, 108, 168, 233, 250, 167, 224, 246, 168, 248, 242, 30, 198, 50268, 25, 31619, 226, 97, 13, 198, 50272, 25, 31619, 233, 117, 168, 233, 254, 167, 237, 226, 23821, 243, 234, 46695, 97, 168, 233, 250, 169, 242, 120, 11, 31619, 233, 112, 167, 108, 108, 167, 232, 242, 220, 169, 237, 238, 168, 243, 242, 166, 111, 120, 23821, 233, 105, 168, 252, 98, 167, 111, 239, 35975, 246, 23821, 96, 120, 168, 248, 242, 23821, 249, 238, 35975, 116, 168, 252, 227, 46695, 230, 46695, 97, 13, 23821, 254, 243, 167, 100, 238, 167, 94, 250, 31619, 223, 232, 168, 250, 120, 168, 227, 242, 168, 243, 120, 220, 47991, 102, 46695, 230, 46695, 97, 13, 220, 198, 50268, 25, 23821, 230, 246, 167, 108, 109, 31619, 110, 230, 23821, 233, 250, 167, 237, 226, 169, 244, 230, 168, 100, 222, 167, 100, 234, 11, 23821, 232, 113, 166, 112, 222, 35975, 226, 31619, 110, 226, 167, 99, 105, 167, 232, 242, 220, 166, 110, 225, 35975, 112, 23821, 244, 112, 167, 254, 113, 168, 232, 113, 46695, 230, 46695, 97, 13, 198, 50272, 25, 23821, 248, 108, 167, 99, 105, 167, 232, 242, 31619, 237, 226, 168, 249, 222, 35975, 112, 31619, 238, 254, 23821, 230, 246, 23821, 252, 230, 167, 232, 242, 23821, 230, 246, 168, 245, 227, 166, 111, 120, 23821, 243, 121, 167, 45539, 167, 241, 97, 35975, 226, 23821, 254, 250, 166, 111, 113, 47991, 246, 166, 111, 254, 23821, 252, 230, 168, 232, 113, 46695, 230, 46695, 97, 13, 31619, 224, 246, 166, 108, 222, 166, 116, 108, 23821, 254, 226, 168, 245, 238, 31619, 235, 242, 31619, 100, 236, 35975, 222, 23821, 254, 243, 167, 111, 112, 167, 98, 120, 31619, 241, 250, 167, 99, 105, 166, 110, 254, 168, 232, 113, 46695, 230, 46695, 97, 13, 198, 50268, 25, 23821, 243, 234, 166, 110, 254, 168, 232, 113, 46695, 230, 46695, 97, 11, 220, 166, 108, 238, 168, 8955, 47991, 102, 46695, 230, 46695, 97, 11, 23821, 251, 246, 168, 8955, 168, 226, 254, 168, 225, 251, 46695, 246, 13]\n",
      "\n",
      "DECODED SENTENCE:\n",
      "#Person1#: ÏïàÎÖïÌïòÏÑ∏Ïöî, Ïä§ÎØ∏Ïä§Ïî®. Ï†ÄÎäî Ìò∏ÌÇ®Ïä§ ÏùòÏÇ¨ÏûÖÎãàÎã§. Ïò§Îäò Ïôú Ïò§ÏÖ®ÎÇòÏöî?\n",
      "#Person2#: Í±¥Í∞ïÍ≤ÄÏßÑÏùÑ Î∞õÎäî Í≤ÉÏù¥ Ï¢ãÏùÑ Í≤É Í∞ôÏïÑÏÑúÏöî.\n",
      "#Person1#: Í∑∏Î†áÍµ∞Ïöî, ÎãπÏã†ÏùÄ 5ÎÖÑ ÎèôÏïà Í±¥Í∞ïÍ≤ÄÏßÑÏùÑ Î∞õÏßÄ ÏïäÏïòÏäµÎãàÎã§. Îß§ÎÖÑ Î∞õÏïÑÏïº Ìï©ÎãàÎã§.\n",
      "#Person2#: ÏïåÍ≥† ÏûàÏäµÎãàÎã§. ÌïòÏßÄÎßå ÏïÑÎ¨¥ Î¨∏Ï†úÍ∞Ä ÏóÜÎã§Î©¥ Ïôú ÏùòÏÇ¨Î•º ÎßåÎÇòÎü¨ Í∞ÄÏïº ÌïòÎÇòÏöî?\n",
      "#Person1#: Ïã¨Í∞ÅÌïú ÏßàÎ≥ëÏùÑ ÌîºÌïòÎäî Í∞ÄÏû• Ï¢ãÏùÄ Î∞©Î≤ïÏùÄ Ïù¥Î•º Ï°∞Í∏∞Ïóê Î∞úÍ≤¨ÌïòÎäî Í≤ÉÏûÖÎãàÎã§. Í∑∏Îü¨Îãà ÎãπÏã†Ïùò Í±¥Í∞ïÏùÑ ÏúÑÌï¥ ÏµúÏÜåÌïú Îß§ÎÖÑ Ìïú Î≤àÏùÄ Ïò§ÏÑ∏Ïöî.\n",
      "#Person2#: ÏïåÍ≤†ÏäµÎãàÎã§.\n",
      "#Person1#: Ïó¨Í∏∞ Î≥¥ÏÑ∏Ïöî. ÎãπÏã†Ïùò ÎààÍ≥º Í∑ÄÎäî Í¥úÏ∞ÆÏïÑ Î≥¥ÏûÖÎãàÎã§. ÍπäÍ≤å Ïà®ÏùÑ Îì§Ïù¥Ïâ¨ÏÑ∏Ïöî. Ïä§ÎØ∏Ïä§Ïî®, Îã¥Î∞∞ ÌîºÏö∞ÏãúÎÇòÏöî?\n",
      "#Person2#: ÎÑ§.\n",
      "#Person1#: ÎãπÏã†ÎèÑ ÏïåÎã§ÏãúÌîº, Îã¥Î∞∞Îäî ÌèêÏïîÍ≥º Ïã¨Ïû•Î≥ëÏùò Ï£ºÏöî ÏõêÏù∏ÏûÖÎãàÎã§. Ï†ïÎßêÎ°ú ÎÅäÏúºÏÖîÏïº Ìï©ÎãàÎã§. \n",
      "#Person2#: ÏàòÎ∞± Î≤à ÏãúÎèÑÌñàÏßÄÎßå, ÏäµÍ¥ÄÏùÑ Î≤ÑÎ¶¨Îäî Í≤ÉÏù¥ Ïñ¥Î†µÏäµÎãàÎã§.\n",
      "#Person1#: Ïö∞Î¶¨Îäî ÎèÑÏõÄÏù¥ Îê† Ïàò ÏûàÎäî ÏàòÏóÖÍ≥º ÏïΩÎ¨ºÎì§ÏùÑ Ï†úÍ≥µÌïòÍ≥† ÏûàÏäµÎãàÎã§. ÎÇòÍ∞ÄÍ∏∞ Ï†ÑÏóê Îçî ÎßéÏùÄ Ï†ïÎ≥¥Î•º ÎìúÎ¶¨Í≤†ÏäµÎãàÎã§.\n",
      "#Person2#: ÏïåÍ≤†ÏäµÎãàÎã§, Í∞êÏÇ¨Ìï©ÎãàÎã§, ÏùòÏÇ¨ÏÑ†ÏÉùÎãò.\n"
     ]
    }
   ],
   "source": [
    "# ÏûëÎèô Ïûò ÎêòÎäîÏßÄ ÌôïÏù∏\n",
    "# Define a test sentence\n",
    "sentence = dataset[\"train\"]['dialogue'][0]\n",
    "\n",
    "\n",
    "# Encode the sentence using the tokenizer, returning PyTorch tensors\n",
    "sentence_encoded = tokenizer(sentence, \n",
    "                            #  max_length=max_source_length, \n",
    "                            #  padding=\"max_length\", #gpt2Î™®Îç∏ÏóêÏÑúÎäî padÍ∞Ä ÏóÜÎã§\n",
    "                             truncation=True, \n",
    "                             add_special_tokens=True)\n",
    "\n",
    "# Decode the encoded sentence, skipping special tokens\n",
    "sentence_decoded = tokenizer.decode(\n",
    "        sentence_encoded[\"input_ids\"], \n",
    "        # max_length=max_target_length, \n",
    "        # padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        add_special_tokens=True,\n",
    "        skip_special_tokens=False\n",
    "    )\n",
    "\n",
    "# Print SENTENCE\n",
    "print('SENTENCE:')\n",
    "print(sentence)\n",
    "\n",
    "# Print the encoded sentence's representation\n",
    "print('\\nENCODED SENTENCE:')\n",
    "print(sentence_encoded[\"input_ids\"])\n",
    "\n",
    "# Print the decoded sentence\n",
    "print('\\nDECODED SENTENCE:')\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing Using Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Zero Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'gemma'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# load model from the hub\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:441\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs_copy\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    439\u001b[0m         _ \u001b[38;5;241m=\u001b[39m kwargs_copy\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 441\u001b[0m     config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:937\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m--> 937\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:643\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content[key]\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    644\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n\u001b[1;32m    645\u001b[0m module_name \u001b[38;5;241m=\u001b[39m model_type_to_module_name(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'gemma'"
     ]
    }
   ],
   "source": [
    "# zero shot\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# load model from the hub\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'gemma'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# load model from the hub\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Define a test sentence\u001b[39;00m\n\u001b[1;32m      9\u001b[0m sentence \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdialogue\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m20\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:441\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs_copy\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    439\u001b[0m         _ \u001b[38;5;241m=\u001b[39m kwargs_copy\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 441\u001b[0m     config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:937\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m--> 937\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:643\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content[key]\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    644\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n\u001b[1;32m    645\u001b[0m module_name \u001b[38;5;241m=\u001b[39m model_type_to_module_name(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'gemma'"
     ]
    }
   ],
   "source": [
    "# zero shot\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# load model from the hub\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "\n",
    "# Define a test sentence\n",
    "sentence = dataset[\"train\"]['dialogue'][20]\n",
    "golden = dataset[\"train\"]['summary'][20]\n",
    "\n",
    "instruction = f\"\"\"\n",
    "ÎåÄÌôî:\n",
    "\n",
    "{sentence}\n",
    "\n",
    "Í∞ÑÍ≤∞ÌïòÍ≤å ÏöîÏïΩÌï¥Ï§ò\n",
    "\"\"\"\n",
    "#instruction = [\"Please summarize the conversation by clearly stating what each speaker did or said. : \" +sentence]\n",
    "# instruction = [\"In this '#Person1#: Hello, Mr. Smith. I'm Dr. Hawkins.' dialogue, the speaker is #Person1#. \\\n",
    "#     Summarize the conversation with a focus on the speakers, ensuring that each speaker's name or identifier, such as #Person1#, is accurately used as the subject in the summary. : \" + sentence]\n",
    "# Encode the sentence using the tokenizer, returning PyTorch tensors\n",
    "sentence_encoded = tokenizer(instruction, \n",
    "                            #  max_length=max_source_length, \n",
    "                            #  padding=\"max_length\", \n",
    "                             truncation=True, \n",
    "                             add_special_tokens=True,\n",
    "                             return_tensors=\"pt\")  # Ensure tensors are returned for model input\n",
    "\n",
    "# Generate the summary using the model\n",
    "summary_ids = model.generate(\n",
    "    sentence_encoded[\"input_ids\"], \n",
    "    # max_length=max_target_length, \n",
    "    min_length=40, \n",
    "    num_beams=5,  # Optional: control the generation strategy\n",
    "    early_stopping=True,  # Optional: stop early when all beams are finished\n",
    "    no_repeat_ngram_size=2\n",
    ")\n",
    "\n",
    "# Decode the encoded sentence, skipping special tokens\n",
    "sentence_decoded = tokenizer.decode(\n",
    "    summary_ids[0],  # Select the first (and usually only) sequence generated\n",
    "    skip_special_tokens=True  # Skip special tokens in the final output\n",
    "    )\n",
    "\n",
    "# Print the encoded sentence's representation\n",
    "print('\\nENCODED SENTENCE:')\n",
    "print(sentence_encoded[\"input_ids\"])\n",
    "\n",
    "# Print the decoded sentence\n",
    "print('\\nDECODED SENTENCE:')\n",
    "print(sentence_decoded)\n",
    "\n",
    "# Print SENTENCE\n",
    "print('\\nGOLDEN:')\n",
    "print(golden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying One Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    for index in example_indices_full:\n",
    "        dialogue = dataset['train']['dialogue_en'][index]\n",
    "        summary = dataset['train']['summary_en'][index]\n",
    "\n",
    "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n",
    "        prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "{summary}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    dialogue = dataset['train']['dialogue_en'][example_index_to_summarize]\n",
    "\n",
    "    prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: We should check in at the Air China counter half an hour before takeoff, Joy.\n",
      "#Person2#: Yeah, I know. The boarding time on the ticket is 17:05, and it's 16:15 now. I think we have enough time.\n",
      "#Person1#: Do we need to show our IDs when we check in?\n",
      "#Person2#: Yeah, that's a must.\n",
      "#Person1#: What about our luggage?\n",
      "#Person2#: We can check in our luggage and carry our small bags in our hands. And we need to open each of them for inspection.\n",
      "#Person1#: Do you think they will search every passenger?\n",
      "#Person2#: I think so. We definitely don't want to have a hijacking incident on the plane today, do we?\n",
      "\n",
      "What was going on?\n",
      "#Person1# asks #Person2# what to do when checking in at the Air China counter.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Can I help you?\n",
      "#Person2#: I'm looking for an MP-3 player. Which brand is the best quality?\n",
      "#Person1#: I recommend Pioneer.\n",
      "#Person2#: Which model is the best seller?\n",
      "#Person1#: This model is very popular with women.\n",
      "#Person2#: Can I see that?\n",
      "#Person1#: Sure, this is multifunctional. Besides playing music, you can also use it to store documents and record.\n",
      "#Person2#: Do you have this model in white?\n",
      "#Person1#: No, but we have it in yellow.\n",
      "#Person2#: I'll take it in yellow then.\n",
      "#Person1#: Please wait a moment. I'll get it for you.\n",
      "#Person2#: OK.\n",
      "\n",
      "What was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [21]\n",
    "example_index_to_summarize = 101\n",
    "\n",
    "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2# is looking for an MP-3 player. #Person1# recommends a pioneer and #Person2# chooses yellow.\n",
      "\n",
      "MODEL GENERATION - ONE SHOT:\n",
      "rien is looking for an MP-3 player. He wants to buy it in yellow.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model understanding more context of the conversation with one shot inference\n",
    "\n",
    "summary = dataset['train']['summary_en'][example_index_to_summarize]\n",
    "\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "#print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "#print(dash_line)\n",
    "print(f'MODEL GENERATION - ONE SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying few Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m example_indices_full \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m21\u001b[39m, \u001b[38;5;241m51\u001b[39m]\n\u001b[1;32m      2\u001b[0m example_index_to_summarize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m101\u001b[39m\n\u001b[0;32m----> 4\u001b[0m few_shot_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mmake_prompt\u001b[49m(example_indices_full, example_index_to_summarize)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(few_shot_prompt)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_prompt' is not defined"
     ]
    }
   ],
   "source": [
    "example_indices_full = [11, 21, 51]\n",
    "example_index_to_summarize = 101\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2# is looking for an MP-3 player. #Person1# recommends a pioneer and #Person2# chooses yellow.\n",
      "\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "rien is looking for an MP-3 player. He wants to buy it in yellow.\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['train']['summary_en'][example_index_to_summarize]\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt', add_special_tokens=True)\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "#print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "#print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Í≤∞Î°†: TOKENIZER ÏàòÏ†ïÎ≥¥Îã§ ONE SHOT, FEW SHOT INFERENCEÍ∞Ä ÎÇ´Îã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 1024)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 16)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 16)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_shot_instruct = \"\"\"Dialogue:\n",
    "\n",
    "#Person1#: We should check in at the Air China counter half an hour before takeoff, Joy.\n",
    "#Person2#: Yeah, I know. The boarding time on the ticket is 17:05, and it's 16:15 now. I think we have enough time.\n",
    "#Person1#: Do we need to show our IDs when we check in?\n",
    "#Person2#: Yeah, that's a must.\n",
    "#Person1#: What about our luggage?\n",
    "#Person2#: We can check in our luggage and carry our small bags in our hands. And we need to open each of them for inspection.\n",
    "#Person1#: Do you think they will search every passenger?\n",
    "#Person2#: I think so. We definitely don't want to have a hijacking incident on the plane today, do we?\n",
    "\n",
    "What was going on?\n",
    "#Person1# asks #Person2# what to do when checking in at the Air China counter.\n",
    "\n",
    "Dialogue:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Îã®Ïñ¥ Ïàò Í≥ÑÏÇ∞\n",
    "word_count = len(one_shot_instruct.split())\n",
    "\n",
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(one_shot_instruct, return_tensors='pt')['input_ids'].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a69eb2ce3fb4688960d25bc3db7be8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "    # one_shot_instructÎ•º ÌÜ†ÌÅ∞ÌôîÌïòÏó¨ ÌÜ†ÌÅ∞ ÏàòÎ•º Í≥ÑÏÇ∞\n",
    "    instruction_tokens = tokenizer(one_shot_instruct, return_tensors='pt')['input_ids'].shape[-1]\n",
    "    \n",
    "    # ÎåÄÌôî ÎÇ¥Ïö©ÏùÑ ÌîÑÎ°¨ÌîÑÌä∏ÏôÄ Í≤∞Ìï©ÌïòÏó¨ ÏûÖÎ†• ÏÉùÏÑ±\n",
    "    inputs = [one_shot_instruct + item + \" What was going on?\" for item in sample[\"dialogue_en\"]]\n",
    "\n",
    "    # max_lengthÏóê ÌîÑÎ°¨ÌîÑÌä∏Ïùò ÌÜ†ÌÅ∞ ÏàòÎ•º Í≥†Î†§Ìïú Í∏∏Ïù¥Î•º ÏÑ§Ï†ï\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length + instruction_tokens, padding=padding, truncation=True, add_special_tokens=True)\n",
    "\n",
    "    # ÌÉÄÍ≤ü(summary_en)ÎèÑ ÌÜ†ÌÅ∞Ìôî\n",
    "    labels = tokenizer(text_target=sample[\"summary_en\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Ìå®Îî© ÌÜ†ÌÅ∞ÏùÑ -100ÏúºÎ°ú ÍµêÏ≤¥ÌïòÏó¨ ÏÜêÏã§ Í≥ÑÏÇ∞Ïóê ÏòÅÌñ•ÏùÑ ÎØ∏ÏπòÏßÄ ÏïäÎèÑÎ°ù Ï≤òÎ¶¨\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖãÏóê Ï†ÑÏ≤òÎ¶¨ Ìï®ÏàòÎ•º Ï†ÅÏö©\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=['fname', 'dialogue', 'summary', 'topic', 'dialogue_en', 'summary_en', 'topic_en'])\n",
    "\n",
    "# Ï≤òÎ¶¨Îêú Îç∞Ïù¥ÌÑ∞ÏÖãÏùò ÌÇ§ Ï∂úÎ†•\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune and evaluate FLAN-T5\n",
    "\n",
    "After we have processed our dataset, we can start training our model. Therefore we first need to load our [FLAN-T5](https://huggingface.co/models?search=flan-t5) from the Hugging Face Hub. In the example we are using a instance with a NVIDIA V100 meaning that we will fine-tune the `base` version of the model. \n",
    "_I plan to do a follow-up post on how to fine-tune the `xxl` version of the model using Deepspeed._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# load model from the hub\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to evaluate our model during training. The `Trainer` supports evaluation during training by providing a `compute_metrics`.  \n",
    "The most commonly used metrics to evaluate summarization task is [rogue_score](https://en.wikipedia.org/wiki/ROUGE_(metric)) short for Recall-Oriented Understudy for Gisting Evaluation). This metric does not behave like the standard accuracy: it will compare a generated summary against a set of reference summaries\n",
    "\n",
    "We are going to use `evaluate` library to evaluate the `rogue` score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/c2/d6/ff9baefc8fc679dcd9eb21b29da3ef10c81aa36be630a7ae78e4611588e1/evaluate-0.4.2-py3-none-any.whl.metadata\n",
      "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.23.5)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.9.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.24.6)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (23.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m706.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /data/ephemeral/home/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# helper function to postprocess text\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    # Ï†ïÏàò Î∞∞Ïó¥Î°ú Î≥ÄÌôòÌïòÍ≥†, Î≤îÏúÑÎ•º tokenizerÏùò vocab ÌÅ¨Í∏∞Î°ú Ï†úÌïú\n",
    "    preds = np.array(preds, dtype=np.int64)\n",
    "    preds = np.clip(preds, 0, tokenizer.vocab_size - 1)\n",
    "    print(preds)\n",
    "  \n",
    "    # ÌÜ†ÌÅ∞ IDÎ•º ÌÖçÏä§Ìä∏Î°ú ÎîîÏΩîÎî©\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    print(decoded_preds)\n",
    "    # ÎùºÎ≤®ÏóêÏÑú -100ÏùÑ Ìå®Îî© ÌÜ†ÌÅ∞ IDÎ°ú ÎåÄÏ≤¥\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞ ÌõÑ Í≤∞Í≥º Î∞òÌôò\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    \n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start training is to create a `DataCollator` that will take care of padding our inputs and labels. We will use the `DataCollatorForSeq2Seq` from the ü§ó Transformers library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to define the hyperparameters (`TrainingArguments`) we want to use for our training. We are leveraging the [Hugging Face Hub](https://huggingface.co/models) integration of the `Trainer` to automatically push our checkpoints, logs and metrics during training into a repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.1)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.40)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.39.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (4.25.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3856150448.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[19], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    wandb login\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "wandb login\n",
    "# 37ef351873d76557e00679959886f35cb3bbc35c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthebestday\u001b[0m (\u001b[33mthebestdayor\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/upstage-nlp-summarization-nlp11/wandb/run-20240904_051058-jhgk8tjh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thebestdayor/huggingface/runs/jhgk8tjh' target=\"_blank\">leafy-sun-2</a></strong> to <a href='https://wandb.ai/thebestdayor/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thebestdayor/huggingface' target=\"_blank\">https://wandb.ai/thebestdayor/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thebestdayor/huggingface/runs/jhgk8tjh' target=\"_blank\">https://wandb.ai/thebestdayor/huggingface/runs/jhgk8tjh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12458' max='249140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 12458/249140 1:09:12 < 21:55:01, 3.00 it/s, Epoch 1/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='499' max='499' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [499/499 05:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OverflowError",
     "evalue": "out of range integral type conversion attempted",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 55\u001b[0m\n\u001b[1;32m     39\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     40\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     41\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     ]\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Training ÏãúÏûë\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1659\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1661\u001b[0m )\n\u001b[0;32m-> 1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2021\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2018\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2020\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2021\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   2024\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   2025\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2287\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2285\u001b[0m             metrics\u001b[38;5;241m.\u001b[39mupdate(dataset_metrics)\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2287\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_seq2seq.py:159\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m gen_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_beams\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    155\u001b[0m     gen_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_beams\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m gen_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_beams\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgeneration_num_beams\n\u001b[1;32m    156\u001b[0m )\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2993\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2990\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   2992\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2993\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2994\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2995\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   2997\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   2998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3001\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3003\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3281\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3277\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[1;32m   3278\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[1;32m   3279\u001b[0m         )\n\u001b[1;32m   3280\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3281\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3282\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3283\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[17], line 25\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_preds)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m     24\u001b[0m     preds \u001b[38;5;241m=\u001b[39m preds[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 25\u001b[0m decoded_preds \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Replace -100 in the labels as we can't decode them.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(labels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m, labels, tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3446\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[0;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[1;32m   3423\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3424\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3427\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3428\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   3429\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3430\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3431\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3444\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m   3447\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[1;32m   3448\u001b[0m             seq,\n\u001b[1;32m   3449\u001b[0m             skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[1;32m   3450\u001b[0m             clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[1;32m   3451\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3452\u001b[0m         )\n\u001b[1;32m   3453\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[1;32m   3454\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3447\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[1;32m   3423\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3424\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3427\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3428\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   3429\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3430\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3431\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3444\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m-> 3447\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3448\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3449\u001b[0m \u001b[43m            \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3450\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3451\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3452\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3453\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[1;32m   3454\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3486\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3483\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3484\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3490\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:549\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    548\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 549\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    552\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    555\u001b[0m )\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[0;31mOverflowError\u001b[0m: out of range integral type conversion attempted"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from huggingface_hub import HfFolder\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "# GPU ÏÇ¨Ïö© Í∞ÄÎä• Ïó¨Î∂Ä ÌôïÏù∏ Î∞è ÏÑ§Ï†ï\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Hugging Face repository id\n",
    "repository_id = f\"{model_id.split('/')[1]}-{dataset_id}\"\n",
    "\n",
    "# Define training args with additional parameters\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    num_train_epochs=20,  # Ï¥ù 20 ÏóêÌè≠ ÎèôÏïà ÌïôÏäµ\n",
    "    learning_rate=1e-5,  # ÌïôÏäµÎ•†\n",
    "    per_device_train_batch_size=1,  # ÌõàÎ†® Ï§ë Ìïú Ïû•ÏπòÎãπ Î∞∞Ïπò ÌÅ¨Í∏∞\n",
    "    per_device_eval_batch_size=1,  # ÌèâÍ∞Ä Ï§ë Ìïú Ïû•ÏπòÎãπ Î∞∞Ïπò ÌÅ¨Í∏∞\n",
    "    warmup_ratio=0.1,  # ÏõåÎ∞çÏóÖ ÎπÑÏú®\n",
    "    weight_decay=0.01,  # Í∞ÄÏ§ëÏπò Í∞êÏá†\n",
    "    lr_scheduler_type='cosine',  # ÏΩîÏÇ¨Ïù∏ Ïä§ÏºÄÏ§ÑÎü¨ ÏÇ¨Ïö©\n",
    "    optim='adamw_torch',  # ÏòµÌã∞ÎßàÏù¥Ï†Ä: AdamW ÏÇ¨Ïö©\n",
    "    gradient_accumulation_steps=1,  # Í∏∞Ïö∏Í∏∞ ÎàÑÏ†Å Îã®Í≥Ñ\n",
    "    evaluation_strategy='epoch',  # ÏóêÌè≠ Îã®ÏúÑÎ°ú ÌèâÍ∞Ä\n",
    "    save_strategy='epoch',  # ÏóêÌè≠ Îã®ÏúÑÎ°ú Ï†ÄÏû•\n",
    "    save_total_limit=5,  # Ï¥ù 5Í∞úÏùò Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Î•º Ï†ÄÏû•\n",
    "    fp16=False,  # mixed precision ÌïôÏäµ ÌôúÏÑ±Ìôî # TrueÎ°ú ÌïòÎ©¥ overflow\n",
    "    load_best_model_at_end=True,  # Í∞ÄÏû• Ï¢ãÏùÄ Î™®Îç∏ÏùÑ ÎßàÏßÄÎßâÏóê Î°úÎìú\n",
    "    seed=42,  # Ïû¨ÌòÑÏÑ±ÏùÑ ÏúÑÌïú ÏãúÎìú Í∞í\n",
    "    logging_dir=\"./logs\",  # Î°úÍ∑∏ ÎîîÎ†âÌÜ†Î¶¨\n",
    "    logging_strategy=\"epoch\",  # ÏóêÌè≠ÎßàÎã§ Î°úÍπÖ\n",
    "    predict_with_generate=True,  # ÏÉùÏÑ± Î™®ÎìúÎ•º ÏÇ¨Ïö©Ìï† Îïå ÌèâÍ∞Ä ÏÑ§Ï†ï\n",
    "    generation_max_length=max_target_length,  # ÏµúÎåÄ ÏÉùÏÑ± Í∏∏Ïù¥\n",
    "    do_train=True,  # ÌïôÏäµ Ïó¨Î∂Ä\n",
    "    do_eval=True,  # ÌèâÍ∞Ä Ïó¨Î∂Ä\n",
    ")\n",
    "\n",
    "# Create Trainer instance with early stopping callback\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"val\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(\n",
    "            early_stopping_patience=3,  # 3Î≤àÏùò ÏóêÌè≠ ÎèôÏïà Í∞úÏÑ†ÎêòÏßÄ ÏïäÏúºÎ©¥ Ï§ëÎã®\n",
    "            early_stopping_threshold=0.001  # ÏÑ±Îä•Ïù¥ 0.001ÎßåÌÅº Í∞úÏÑ†ÎêòÏßÄ ÏïäÏúºÎ©¥ Ï§ëÎã®\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Training ÏãúÏûë\n",
    "trainer.train()\n",
    "# 20 epoch: 13ÏãúÍ∞Ñ ÏòàÏÉÅ\n",
    "# 1 epoch: 0.69ÏãúÍ∞Ñ ÏòàÏÉÅ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start our training by using the `train` method of the `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='228' max='62285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  228/62285 01:11 < 5:29:24, 3.14 it/s, Epoch 0.02/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1659\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1661\u001b[0m )\n\u001b[0;32m-> 1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1927\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1928\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1929\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1932\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1933\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1934\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1935\u001b[0m ):\n\u001b[1;32m   1936\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1937\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2717\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2715\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m   2716\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2717\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2719\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "#2.16Ïùº ÏòàÏÉÅ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![flan-t5-tensorboard](../assets/flan-t5-tensorboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we have trained our model. üéâ Lets run evaluate the best model again on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 819\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='103' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [103/103 01:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.3715944290161133,\n",
       " 'eval_rouge1': 47.2358,\n",
       " 'eval_rouge2': 23.5135,\n",
       " 'eval_rougeL': 39.6266,\n",
       " 'eval_rougeLsum': 43.3458,\n",
       " 'eval_gen_len': 17.39072039072039,\n",
       " 'eval_runtime': 108.99,\n",
       " 'eval_samples_per_second': 7.514,\n",
       " 'eval_steps_per_second': 0.945,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score we achieved is an `rouge1` score of `47.23`. \n",
    "\n",
    "Lets save our results and tokenizer to the Hugging Face Hub and create a model card. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our tokenizer and create model card\n",
    "tokenizer.save_pretrained(repository_id)\n",
    "trainer.create_model_card()\n",
    "# Push the results to the hub\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Inference\n",
    "\n",
    "Now we have a trained model, we can use it to run inference. We will use the `pipeline` API from transformers and a `test` example from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialogue: \n",
      "Abby: Have you talked to Miro?\n",
      "Dylan: No, not really, I've never had an opportunity\n",
      "Brandon: me neither, but he seems a nice guy\n",
      "Brenda: you met him yesterday at the party?\n",
      "Abby: yes, he's so interesting\n",
      "Abby: told me the story of his father coming from Albania to the US in the early 1990s\n",
      "Dylan: really, I had no idea he is Albanian\n",
      "Abby: he is, he speaks only Albanian with his parents\n",
      "Dylan: fascinating, where does he come from in Albania?\n",
      "Abby: from the seacoast\n",
      "Abby: Duress I believe, he told me they are not from Tirana\n",
      "Dylan: what else did he tell you?\n",
      "Abby: That they left kind of illegally\n",
      "Abby: it was a big mess and extreme poverty everywhere\n",
      "Abby: then suddenly the border was open and they just left \n",
      "Abby: people were boarding available ships, whatever, just to get out of there\n",
      "Abby: he showed me some pictures, like <file_photo>\n",
      "Dylan: insane\n",
      "Abby: yes, and his father was among the people\n",
      "Dylan: scary but interesting\n",
      "Abby: very!\n",
      "---------------\n",
      "flan-t5-base summary:\n",
      "Abby met Miro yesterday at the party. Miro's father came from Albania to the US in the early 1990s. He speaks Albanian with his parents. The border was open and people were boarding ships to get out of there.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from random import randrange        \n",
    "\n",
    "# load model and tokenizer from huggingface hub with pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"philschmid/flan-t5-base-samsum\", device=0)\n",
    "\n",
    "# select a random test sample\n",
    "sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n",
    "print(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n",
    "\n",
    "# summarize dialogue\n",
    "res = summarizer(sample[\"dialogue\"])\n",
    "\n",
    "print(f\"flan-t5-base summary:\\n{res[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
