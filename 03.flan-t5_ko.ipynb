{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Development Environment\n",
    "\n",
    "Our first step is to install the Hugging Face Libraries, including transformers and datasets. Running the following cell will install all the required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "GPU device name: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "    print(f\"GPU device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU is not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: pytesseract in /opt/conda/lib/python3.10/site-packages (0.3.13)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.28.0)\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/75/35/07c9879163b603f0e464b0f6e6e628a2340cfc7cdc5ca8e7d52d776710d4/transformers-4.44.2-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\n",
      "Requirement already satisfied: rouge-score in /opt/conda/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.17.1)\n",
      "Requirement already satisfied: py7zr in /opt/conda/lib/python3.10/site-packages (0.22.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from pytesseract) (23.1)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from pytesseract) (9.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.20,>=0.19 from https://files.pythonhosted.org/packages/40/4f/eb78de4af3b17b589f43a369cbf0c3a7173f25c3d2cd93068852c07689aa/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# python\n",
    "!pip install pytesseract transformers datasets rouge-score nltk tensorboard py7zr --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf159e534374598b71825a73213fa5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "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\n",
    "T5_DialogueSum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare dialogueSum dataset from local\n",
    "- This DialogueSum dataset was originally in English but was translated into Korean by teachers using the Solar API for educational purposes. However, the translation seemed somewhat unnatural for native Korean speakers, so I used the Solar API to retranslate it into English to facilitate a more accurate summarization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the `dialogueSum` dataset, we use the `load_dataset()` method from the 🤗 Datasets library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"dialoguSum_Solar_koen\"\n",
    "# huggingface hub model id\n",
    "# model_id=\"paust/pko-flan-t5-large\" # 메모리부족\n",
    "#model_id=\"paust/pko-t5-large\"# 메모리부족\n",
    "model_id=\"paust/pko-t5-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 12457\n",
      "val dataset size: 499\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset('csv', data_files={'train': \"/data/ephemeral/home/data/train_en.csv\", 'val': \"/data/ephemeral/home/data/dev_en.csv\"})\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"val dataset size: {len(dataset['val'])}\")\n",
    "\n",
    "# Train dataset size: 12457\n",
    "# Test dataset size: 499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['fname', 'dialogue', 'summary', 'topic', 'dialogue_en', 'summary_en', 'topic_en'],\n",
       "    num_rows: 12457\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets checkout an example of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialogue: \n",
      "#Person1#: 할아버지, 이 식당은 정말 오래된 것 같아요! 언제 지어진 건가요?#Person2#: 정확히 언제 개업했는지는 모르겠네. 하지만 나보다 더 오래된 건 확실해. 어릴 적에 여기서 자주 밥을 먹었거든.#Person1#: 오, 흥미롭네요. 아빠가 어렸을 때 할아버지가 여기에 데려왔다고 들었어요. 이제 제 차례인가봐요.#Person2#: 그래. 우리는 여기 VIP 고객이지. 오늘 뭐 먹고 싶어?#Person1#: 햄버거랑 코카콜라 한 병 먹고 싶어요.#Person2#: 이녀석아, 그건 아니지. 여긴 중국 식당이야. 전통적인 걸 먹어봐.#Person1#: 그럼 특별히 먹고 싶은 건 없어요. 할아버지, 여기서 가장 좋아하는 메뉴는 뭔가요?#Person2#: 중국식 떡볶이야. 네 아빠도 좋아하더군. 한번 먹어볼래?#Person1#: 그럼요. 달달한 건가요?#Person2#: 그럼, 당연하지. 하지만 짠 맛도 만들어 줄 수 있을 거야.#Person1#: 좋아요, 매운 걸로 해주세요.#Person2#: 이봐. 너무 무리하지마.#Person1#: 특별하게 먹고 싶어요. 그리고 제가 항상 매운 음식을 좋아한다는 건 아시잖아요.#Person2#: 그럼 그렇게 하지. 빨간 고추 좀 넣어달라고 부탁해야겠네.\n",
      "---------------\n",
      "summary: \n",
      "할아버지가 #Person1#을 오래된 중국 식당에 데려가서 중국식 떡볶이를 추천한다. #Person1#은 특별하게 먹고 싶어해서 할아버지가 식당에 빨간 고추를 넣어달라고 부탁할 것이다.\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "from random import randrange        \n",
    "\n",
    "\n",
    "sample = dataset['train'][randrange(len(dataset[\"train\"]))]\n",
    "print(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n",
    "print(f\"summary: \\n{sample['summary']}\\n---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5TokenizerFast\n",
    "# Load tokenizer of FLAN-t5-base\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 1791\n",
      "Min source length: 70\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ef8a19aa364d2babf38049348b0a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12956 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 319\n",
      "Min target length: 14\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# The maximum total input sequence length after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"val\"]]).map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "min_source_length = min([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Min source length: {min_source_length}\")\n",
    "\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"val\"]]).map(lambda x: tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "print(f\"Max target length: {max_target_length}\")\n",
    "min_target_length = min([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "print(f\"Min target length: {min_target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'#CarNumber#' is not in the vocabulary.\n",
      "'#SSN#' is not in the vocabulary.\n",
      "'#PhoneNumber#' is not in the vocabulary.\n",
      "'#PassportNumber#' is not in the vocabulary.\n",
      "'#Email#' is not in the vocabulary.\n",
      "'#CardNumber#' is not in the vocabulary.\n",
      "'#Address#' is not in the vocabulary.\n",
      "'#DateOfBirth#' is not in the vocabulary.\n",
      "'#Person4#' is not in the vocabulary.\n",
      "'#Person7#' is not in the vocabulary.\n",
      "'#Person3#' is not in the vocabulary.\n",
      "'#Person2#' is not in the vocabulary.\n",
      "'#Person#' is not in the vocabulary.\n",
      "'#Person6#' is not in the vocabulary.\n",
      "'#Person5#' is not in the vocabulary.\n",
      "'#Person1#' is not in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "special_tokens = ['#CarNumber#', '#SSN#', '#PhoneNumber#', '#PassportNumber#', '#Email#', '#CardNumber#', '#Address#', '#DateOfBirth#', \\\n",
    "'#Person4#', '#Person7#', '#Person3#', '#Person2#', '#Person#', '#Person6#', '#Person5#', '#Person1#']\n",
    "for token in special_tokens:\n",
    "    if token in tokenizer.get_vocab():\n",
    "        print(f\"'{token}' is already in the vocabulary.\")\n",
    "    else:\n",
    "        print(f\"'{token}' is not in the vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab size: 50358\n",
      "New vocab size: 50374\n"
     ]
    }
   ],
   "source": [
    "original_vocab_size = len(tokenizer)\n",
    "\n",
    "special_tokens = ['#CarNumber#', '#SSN#', '#PhoneNumber#', '#PassportNumber#', '#Email#', '#CardNumber#', '#Address#', '#DateOfBirth#', \\\n",
    "'#Person4#', '#Person7#', '#Person3#', '#Person2#', '#Person#', '#Person6#', '#Person5#', '#Person1#']\n",
    "#tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
    "tokenizer.add_tokens(special_tokens)\n",
    "new_vocab_size = len(tokenizer)\n",
    "\n",
    "print(f\"Original vocab size: {original_vocab_size}\")\n",
    "print(f\"New vocab size: {new_vocab_size}\")\n",
    "\n",
    "# Original vocab size: 50358\n",
    "# New vocab size: 50374"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizer 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE:\n",
      "#Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?\n",
      "#Person2#: 건강검진을 받는 것이 좋을 것 같아서요.\n",
      "#Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 않았습니다. 매년 받아야 합니다.\n",
      "#Person2#: 알고 있습니다. 하지만 아무 문제가 없다면 왜 의사를 만나러 가야 하나요?\n",
      "#Person1#: 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 오세요.\n",
      "#Person2#: 알겠습니다.\n",
      "#Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 피우시나요?\n",
      "#Person2#: 네.\n",
      "#Person1#: 당신도 알다시피, 담배는 폐암과 심장병의 주요 원인입니다. 정말로 끊으셔야 합니다. \n",
      "#Person2#: 수백 번 시도했지만, 습관을 버리는 것이 어렵습니다.\n",
      "#Person1#: 우리는 도움이 될 수 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다.\n",
      "#Person2#: 알겠습니다, 감사합니다, 의사선생님.\n",
      "\n",
      "ENCODED SENTENCE:\n",
      "[50373, 27, 222, 1381, 963, 13, 222, 14563, 796, 15, 222, 425, 274, 222, 528, 21693, 222, 2183, 535, 15, 222, 805, 222, 997, 222, 29297, 296, 32, 200, 50369, 27, 222, 1323, 6831, 291, 222, 1760, 222, 398, 262, 222, 2150, 222, 398, 222, 2585, 296, 15, 200, 50373, 27, 222, 5367, 296, 13, 222, 1837, 311, 222, 22, 482, 222, 1063, 222, 1323, 6831, 291, 222, 3629, 222, 6136, 15, 222, 4376, 222, 5467, 222, 550, 15, 200, 50369, 27, 222, 1500, 222, 738, 15, 222, 890, 222, 1129, 222, 871, 278, 222, 7203, 222, 997, 222, 2183, 333, 222, 14123, 222, 2240, 222, 42536, 32, 200, 50373, 27, 222, 6747, 222, 5153, 291, 222, 15508, 222, 985, 222, 810, 222, 933, 311, 222, 16469, 222, 434, 3945, 222, 2384, 429, 222, 398, 535, 15, 222, 5380, 222, 1837, 302, 222, 1323, 291, 222, 863, 222, 6252, 222, 4376, 222, 305, 222, 490, 311, 222, 4669, 15, 200, 50369, 27, 222, 7587, 15, 200, 50373, 27, 222, 1005, 222, 1473, 15, 222, 1837, 302, 222, 803, 381, 222, 886, 274, 222, 4193, 222, 3912, 15, 222, 9494, 222, 1664, 291, 222, 3708, 26309, 15, 222, 14563, 796, 13, 222, 4265, 222, 23572, 2956, 296, 32, 200, 50369, 27, 222, 449, 15, 200, 50373, 27, 222, 1837, 301, 222, 33914, 13, 222, 4265, 274, 222, 1393, 1105, 381, 222, 4245, 761, 302, 222, 2293, 222, 2779, 535, 15, 222, 861, 293, 222, 2567, 17010, 222, 550, 15, 222, 200, 50369, 27, 222, 9376, 222, 490, 222, 3156, 1832, 13, 222, 2370, 291, 222, 6016, 222, 398, 262, 222, 8918, 15, 200, 50373, 27, 222, 699, 274, 222, 1436, 262, 222, 764, 222, 334, 222, 491, 222, 1796, 381, 222, 6936, 5672, 222, 1239, 443, 222, 738, 15, 222, 9272, 222, 353, 279, 222, 451, 222, 882, 222, 889, 333, 222, 2793, 15, 200, 50369, 27, 222, 7587, 13, 222, 835, 550, 13, 222, 2183, 1746, 549, 15, 1]\n",
      "\n",
      "DECODED SENTENCE:\n",
      "#Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?\n",
      "#Person2#: 건강검진을 받는 것이 좋을 것 같아서요.\n",
      "#Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 않았습니다. 매년 받아야 합니다.\n",
      "#Person2#: 알고 있습니다. 하지만 아무 문제가 없다면 왜 의사를 만나러 가야 하나요?\n",
      "#Person1#: 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 오세요.\n",
      "#Person2#: 알겠습니다.\n",
      "#Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 피우시나요?\n",
      "#Person2#: 네.\n",
      "#Person1#: 당신도 알다시피, 담배는 폐암과 심장병의 주요 원인입니다. 정말로 끊으셔야 합니다. \n",
      "#Person2#: 수백 번 시도했지만, 습관을 버리는 것이 어렵습니다.\n",
      "#Person1#: 우리는 도움이 될 수 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다.\n",
      "#Person2#: 알겠습니다, 감사합니다, 의사선생님.\n"
     ]
    }
   ],
   "source": [
    "# 작동 잘 되는지 확인\n",
    "# Define a test sentence\n",
    "sentence = dataset[\"train\"]['dialogue'][0]\n",
    "\n",
    "\n",
    "# Encode the sentence using the tokenizer, returning PyTorch tensors\n",
    "sentence_encoded = tokenizer(sentence, \n",
    "                             truncation=True, \n",
    "                             add_special_tokens=True)\n",
    "\n",
    "# Decode the encoded sentence, skipping special tokens\n",
    "sentence_decoded = tokenizer.decode(\n",
    "        sentence_encoded[\"input_ids\"], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "# Print SENTENCE\n",
    "print('SENTENCE:')\n",
    "print(sentence)\n",
    "\n",
    "# Print the encoded sentence's representation\n",
    "print('\\nENCODED SENTENCE:')\n",
    "print(sentence_encoded[\"input_ids\"])\n",
    "\n",
    "# Print the decoded sentence\n",
    "print('\\nDECODED SENTENCE:')\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing Using Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Zero Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(195808, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(195808, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(195808, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=195808, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab size: 50374\n",
      "New vocab size: 195808\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get the original vocabulary size\n",
    "original_vocab_size = len(tokenizer)\n",
    "print(f\"Original vocab size: {original_vocab_size}\")\n",
    "\n",
    "# Define a function to extract unique words from text\n",
    "def extract_unique_words(dataset_column):\n",
    "    unique_words = set()\n",
    "    for sentence in dataset_column:\n",
    "        words = sentence.split()  # Simple split, adjust with tokenizer if needed\n",
    "        unique_words.update(words)\n",
    "    return unique_words\n",
    "\n",
    "# Step 1: Extract unique words from the dataset\n",
    "unique_words_train_dialogue = extract_unique_words(dataset['train']['dialogue'])\n",
    "unique_words_train_summary = extract_unique_words(dataset['train']['summary'])\n",
    "unique_words_val_dialogue = extract_unique_words(dataset['val']['dialogue'])\n",
    "unique_words_val_summary = extract_unique_words(dataset['val']['summary'])\n",
    "\n",
    "# Step 2: Extract unique words from the test set\n",
    "test = pd.read_csv('/data/ephemeral/home/data/test.csv')\n",
    "unique_words_test_dialogue = extract_unique_words(test['dialogue'])\n",
    "\n",
    "# Combine all unique words\n",
    "all_unique_words = unique_words_train_dialogue | unique_words_train_summary | unique_words_val_dialogue | unique_words_val_summary | unique_words_test_dialogue\n",
    "\n",
    "# Step 3: Add these unique words to the tokenizer vocabulary\n",
    "tokenizer.add_tokens(list(set(all_unique_words)))\n",
    "\n",
    "# Step 4: Check the new vocabulary size\n",
    "new_vocab_size = len(tokenizer)\n",
    "print(f\"New vocab size: {new_vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Zero Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ENCODED SENTENCE:\n",
      "tensor([[ 7675,    78, 20359,    74, 26159,    27,   222, 50373,    27,   222,\n",
      "          1381,   963,    13,   222, 14563,   796,    15,   222,   425,   274,\n",
      "           222,   528, 21693,   222,  2183,   535,    15,   222,   805,   222,\n",
      "           997,   222, 29297,   296,    32,   200, 50369,    27,   222,  1323,\n",
      "          6831,   291,   222,  1760,   222,   398,   262,   222,  2150,   222,\n",
      "           398,   222,  2585,   296,    15,   200, 50373,    27,   222,  5367,\n",
      "           296,    13,   222,  1837,   311,   222,    22,   482,   222,  1063,\n",
      "           222,  1323,  6831,   291,   222,  3629,   222,  6136,    15,   222,\n",
      "          4376,   222,  5467,   222,   550,    15,   200, 50369,    27,   222,\n",
      "          1500,   222,   738,    15,   222,   890,   222,  1129,   222,   871,\n",
      "           278,   222,  7203,   222,   997,   222,  2183,   333,   222, 14123,\n",
      "           222,  2240,   222, 42536,    32,   200, 50373,    27,   222,  6747,\n",
      "           222,  5153,   291,   222, 15508,   222,   985,   222,   810,   222,\n",
      "           933,   311,   222, 16469,   222,   434,  3945,   222,  2384,   429,\n",
      "           222,   398,   535,    15,   222,  5380,   222,  1837,   302,   222,\n",
      "          1323,   291,   222,   863,   222,  6252,   222,  4376,   222,   305,\n",
      "           222,   490,   311,   222,  4669,    15,   200, 50369,    27,   222,\n",
      "          7587,    15,   200, 50373,    27,   222,  1005,   222,  1473,    15,\n",
      "           222,  1837,   302,   222,   803,   381,   222,   886,   274,   222,\n",
      "          4193,   222,  3912,    15,   222,  9494,   222,  1664,   291,   222,\n",
      "          3708, 26309,    15,   222, 14563,   796,    13,   222,  4265,   222,\n",
      "         23572,  2956,   296,    32,   200, 50369,    27,   222,   449,    15,\n",
      "           200, 50373,    27,   222,  1837,   301,   222, 33914,    13,   222,\n",
      "          4265,   274,   222,  1393,  1105,   381,   222,  4245,   761,   302,\n",
      "           222,  2293,   222,  2779,   535,    15,   222,   861,   293,   222,\n",
      "          2567, 17010,   222,   550,    15,   222,   200, 50369,    27,   222,\n",
      "          9376,   222,   490,   222,  3156,  1832,    13,   222,  2370,   291,\n",
      "           222,  6016,   222,   398,   262,   222,  8918,    15,   200, 50373,\n",
      "            27,   222,   699,   274,   222,  1436,   262,   222,   764,   222,\n",
      "           334,   222,   491,   222,  1796,   381,   222,  6936,  5672,   222,\n",
      "          1239,   443,   222,   738,    15,   222,  9272,   222,   353,   279,\n",
      "           222,   451,   222,   882,   222,   889,   333,   222,  2793,    15,\n",
      "           200, 50369,    27,   222,  7587,    13,   222,   835,   550,    13,\n",
      "           222,  2183,  1746,   549,    15,     1]])\n",
      "\n",
      "DECODED SENTENCE:\n",
      " 스미스씨, 저는 호킨스 의사입니다.\n",
      "스미스: 감사합니다.\n",
      "\n",
      "\n",
      " 당신의 건강을 위해 오세요. 그리고\n",
      "\n",
      "부록 \n",
      "사진: :\n",
      "\n",
      "\n",
      "\n",
      " \n",
      ".\n",
      " \n",
      " \n",
      "  \n",
      "\n",
      " \n",
      "\n",
      "\n",
      " 네,\n",
      "병원 대기 중인 환자들에게 전화를 걸었습니다.\n",
      "\n",
      "GOLDEN:\n",
      "스미스씨가 건강검진을 받고 있고, 호킨스 의사는 매년 건강검진을 받는 것을 권장합니다. 호킨스 의사는 스미스씨가 담배를 끊는 데 도움이 될 수 있는 수업과 약물에 대한 정보를 제공할 것입니다.\n"
     ]
    }
   ],
   "source": [
    "# zero shot\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "# load model from the hub\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_id)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "# Define a test sentence\n",
    "sentence = dataset[\"train\"]['dialogue'][0]\n",
    "golden = dataset[\"train\"]['summary'][0]\n",
    "\n",
    "instruction = \"summarize: \"+sentence\n",
    "\n",
    "#instruction = [\"Please summarize the conversation by clearly stating what each speaker did or said. : \" +sentence]\n",
    "# instruction = [\"In this '#Person1#: Hello, Mr. Smith. I'm Dr. Hawkins.' dialogue, the speaker is #Person1#. \\\n",
    "#     Summarize the conversation with a focus on the speakers, ensuring that each speaker's name or identifier, such as #Person1#, is accurately used as the subject in the summary. : \" + sentence]\n",
    "# Encode the sentence using the tokenizer, returning PyTorch tensors\n",
    "input_ids = tokenizer(instruction, \n",
    "                    max_length=max_source_length+20, \n",
    "                    padding=\"longest\", \n",
    "                    #truncation=True, \n",
    "                    add_special_tokens=True,\n",
    "                    return_tensors=\"pt\").input_ids  # Ensure tensors are returned for model input\n",
    "\n",
    "\n",
    "# Generate the summary using the model\n",
    "summary_ids = model.generate(\n",
    "    input_ids, \n",
    "    max_length=max_target_length, \n",
    "    min_length=100, \n",
    "    num_beams=10,  # Optional: control the generation strategy\n",
    "    early_stopping=True,  # Optional: stop early when all beams are finished\n",
    "    no_repeat_ngram_size=2\n",
    ")\n",
    "\n",
    "# Decode the encoded sentence, skipping special tokens\n",
    "summary = tokenizer.decode(\n",
    "    summary_ids[0],  # Select the first (and usually only) sequence generated\n",
    "    skip_special_tokens=True  # Skip special tokens in the final output\n",
    "    )\n",
    "\n",
    "# Print the encoded sentence's representation\n",
    "# print('\\nENCODED SENTENCE:')\n",
    "# print(input_ids)\n",
    "\n",
    "# Print the decoded sentence\n",
    "print('\\nDECODED SENTENCE:')\n",
    "print(summary)\n",
    "\n",
    "# Print SENTENCE\n",
    "print('\\nGOLDEN:')\n",
    "print(golden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying One Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    for index in example_indices_full:\n",
    "        dialogue = dataset['train']['dialogue'][index]\n",
    "        summary = dataset['train']['summary'][index]\n",
    "\n",
    "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n",
    "        prompt += f\"\"\"대화:{dialogue}\\n대화내용요약:{summary}\"\"\"\n",
    "\n",
    "    dialogue = dataset['train']['dialogue'][example_index_to_summarize]\n",
    "\n",
    "    prompt += f\"\"\"대화:{dialogue}\\n대화내용요약:\"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대화:#Person1#: 안녕하세요, 파커 부인, 어떻게 지내셨나요?\n",
      "#Person2#: 안녕하세요, 피터스 박사님. 잘 지냈습니다, 감사합니다. 리키와 함께 백신 접종을 위해 왔습니다.\n",
      "#Person1#: 좋습니다. 백신 접종 기록을 보니, 리키는 이미 소아마비, 디프테리아, B형 간염 백신을 맞았군요. 그는 14개월이므로, 이제 A형 간염, 수두, 홍역 백신을 맞아야 합니다.\n",
      "#Person2#: 풍진과 볼거리는 어떻게 되나요?\n",
      "#Person1#: 지금은 이 백신들만 접종할 수 있고, 몇 주 후에 나머지를 접종할 수 있습니다.\n",
      "#Person2#: 좋습니다. 박사님, 저도 디프테리아 예방접종이 필요할 것 같아요. 마지막으로 맞은 게 아마도 15년 전이었던 것 같아요!\n",
      "#Person1#: 저희가 기록을 확인하고 간호사에게 부스터를 접종하도록 하겠습니다. 이제, 리키의 팔을 꽉 잡아주세요, 조금 찌릿할 수 있습니다.\n",
      "대화내용요약:파커 부인이 리키를 데리고 백신 접종을 하러 갔다. 피터스 박사는 기록을 확인한 후 리키에게 백신을 접종했다.대화:#Person1#: 도와드릴까요?\n",
      "#Person2#: MP-3 플레이어를 찾고 있어요. 어떤 브랜드가 가장 품질이 좋나요?\n",
      "#Person1#: 파이오니어를 추천드립니다.\n",
      "#Person2#: 어떤 모델이 가장 잘 팔리나요?\n",
      "#Person1#: 이 모델이 여성들에게 매우 인기가 있습니다.\n",
      "#Person2#: 그것을 볼 수 있을까요?\n",
      "#Person1#: 물론입니다, 이것은 다기능입니다. 음악을 재생하는 것 외에도 문서를 저장하고 녹음하는 데도 사용할 수 있습니다.\n",
      "#Person2#: 이 모델을 흰색으로 가지고 계신가요?\n",
      "#Person1#: 아니요, 하지만 노란색은 있습니다.\n",
      "#Person2#: 그럼 노란색으로 할게요.\n",
      "#Person1#: 잠시만 기다려주세요. 가져다 드리겠습니다.\n",
      "#Person2#: 알겠어요.\n",
      "대화내용요약:\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [1]\n",
    "example_index_to_summarize = 101\n",
    "\n",
    "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2#는 MP-3 플레이어를 찾고 있습니다. #Person1#는 파이오니어를 추천하고 #Person2#는 노란색을 선택합니다.\n",
      "\n",
      "MODEL GENERATION - ONE SHOT:\n",
      "20대여성: #@시스템#사진#\n",
      "30대남성:..??!!?\n",
      "40대기혼:,,?,\n",
      "50대미혼:-_--,-/-ᅳᅳ-;;,//;::;\n",
      "60대임부:/_/:__ᅳ\n",
      "70대임산부도 괜찮으려나? ᄏᄏ\n",
      "200명중 한명은 괜찮은데 나머지는 아예 안보여서 아쉽더라 ᅲᅲ 흑흑 ᅮᅮ\n",
      "190명 중에 160명이 과체중이라 그런지 몸무게가 엄청 무거웠어요 ^^ ᄒ\n",
      "150명의 임산부는 다이어트 열심히 하고 있대요. 허허허.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model understanding more context of the conversation with one shot inference\n",
    "\n",
    "summary = dataset['train']['summary'][example_index_to_summarize]\n",
    "\n",
    "inputs = tokenizer(one_shot_prompt, #truncation=True, \n",
    "                             add_special_tokens=True,\n",
    "                             return_tensors=\"pt\")\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=max_target_length,\n",
    "        min_length=40, \n",
    "        num_beams=5,  # Optional: control the generation strategy\n",
    "        early_stopping=True,  # Optional: stop early when all beams are finished\n",
    "        no_repeat_ngram_size=2\n",
    "        )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "#print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "#print(dash_line)\n",
    "print(f'MODEL GENERATION - ONE SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying few Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m example_indices_full \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m21\u001b[39m, \u001b[38;5;241m51\u001b[39m]\n\u001b[1;32m      2\u001b[0m example_index_to_summarize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m101\u001b[39m\n\u001b[0;32m----> 4\u001b[0m few_shot_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mmake_prompt\u001b[49m(example_indices_full, example_index_to_summarize)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(few_shot_prompt)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_prompt' is not defined"
     ]
    }
   ],
   "source": [
    "example_indices_full = [11, 21, 51]\n",
    "example_index_to_summarize = 101\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2# is looking for an MP-3 player. #Person1# recommends a pioneer and #Person2# chooses yellow.\n",
      "\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "rien is looking for an MP-3 player. He wants to buy it in yellow.\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['train']['summary_en'][example_index_to_summarize]\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt', add_special_tokens=True)\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "#print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "#print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 결론: TOKENIZER 수정보다 ONE SHOT, FEW SHOT INFERENCE가 낫다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(50358, 1024)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(50358, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 16)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(50358, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 16)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=50358, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_shot_instruct = \"\"\"Dialogue:\n",
    "\n",
    "#Person1#: We should check in at the Air China counter half an hour before takeoff, Joy.\n",
    "#Person2#: Yeah, I know. The boarding time on the ticket is 17:05, and it's 16:15 now. I think we have enough time.\n",
    "#Person1#: Do we need to show our IDs when we check in?\n",
    "#Person2#: Yeah, that's a must.\n",
    "#Person1#: What about our luggage?\n",
    "#Person2#: We can check in our luggage and carry our small bags in our hands. And we need to open each of them for inspection.\n",
    "#Person1#: Do you think they will search every passenger?\n",
    "#Person2#: I think so. We definitely don't want to have a hijacking incident on the plane today, do we?\n",
    "\n",
    "What was going on?\n",
    "#Person1# asks #Person2# what to do when checking in at the Air China counter.\n",
    "\n",
    "Dialogue:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 수 계산\n",
    "word_count = len(one_shot_instruct.split())\n",
    "\n",
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(one_shot_instruct, return_tensors='pt')['input_ids'].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a69eb2ce3fb4688960d25bc3db7be8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "    # one_shot_instruct를 토큰화하여 토큰 수를 계산\n",
    "    instruction_tokens = tokenizer(one_shot_instruct, return_tensors='pt')['input_ids'].shape[-1]\n",
    "    \n",
    "    # 대화 내용을 프롬프트와 결합하여 입력 생성\n",
    "    inputs = [one_shot_instruct + item + \" What was going on?\" for item in sample[\"dialogue_en\"]]\n",
    "\n",
    "    # max_length에 프롬프트의 토큰 수를 고려한 길이를 설정\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length + instruction_tokens, padding=padding, truncation=True, add_special_tokens=True)\n",
    "\n",
    "    # 타겟(summary_en)도 토큰화\n",
    "    labels = tokenizer(text_target=sample[\"summary_en\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # 패딩 토큰을 -100으로 교체하여 손실 계산에 영향을 미치지 않도록 처리\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# 데이터셋에 전처리 함수를 적용\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=['fname', 'dialogue', 'summary', 'topic', 'dialogue_en', 'summary_en', 'topic_en'])\n",
    "\n",
    "# 처리된 데이터셋의 키 출력\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune and evaluate FLAN-T5\n",
    "\n",
    "After we have processed our dataset, we can start training our model. Therefore we first need to load our [FLAN-T5](https://huggingface.co/models?search=flan-t5) from the Hugging Face Hub. In the example we are using a instance with a NVIDIA V100 meaning that we will fine-tune the `base` version of the model. \n",
    "_I plan to do a follow-up post on how to fine-tune the `xxl` version of the model using Deepspeed._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a671ef2cc5e4d1a90f4ff82062e23d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels[\"input_ids\"][0]: [14563, 796, 278, 222, 1323, 6831, 291, 222, 1475, 222, 1110, 13, 222, 528, 21693, 222, 2183, 274, 222, 4376, 222, 1323, 6831, 291, 222, 1760, 222, 398, 291, 222, 6669, 550, 15, 222, 528, 21693, 222, 2183, 274, 222, 14563, 796, 278, 222, 4265, 333, 222, 24367, 222, 374, 222, 1436, 262, 222, 764, 222, 334, 222, 491, 222, 1796, 381, 222, 6936, 279, 222, 745, 222, 889, 333, 222, 1239, 411, 222, 398, 535, 15, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "labels[\"input_ids\"][0]: [50373, 381, 222, 50369, 274, 222, 5758, 302, 222, 642, 279, 222, 2581, 222, 2001, 279, 222, 1162, 222, 1170, 588, 15, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "labels[\"input_ids\"][0]: [50373, 311, 222, 2190, 655, 222, 969, 279, 222, 912, 305, 222, 1044, 222, 797, 222, 4169, 222, 2333, 291, 222, 312, 3100, 222, 7493, 2559, 222, 550, 15, 222, 1082, 690, 222, 2190, 278, 222, 17161, 890, 222, 2356, 690, 222, 3610, 550, 15, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "labels[\"input_ids\"][0]: [50373, 311, 222, 50369, 655, 222, 5642, 437, 222, 3879, 293, 222, 975, 222, 23569, 620, 279, 222, 745, 222, 889, 333, 222, 35463, 15, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "labels[\"input_ids\"][0]: [50373, 311, 222, 1203, 293, 222, 5056, 222, 613, 7440, 3915, 655, 222, 3500, 291, 222, 1085, 222, 1044, 222, 797, 222, 1141, 291, 222, 550, 15, 222, 613, 7440, 3915, 274, 222, 50373, 655, 222, 337, 302, 222, 1216, 13, 222, 1016, 222, 1410, 13, 222, 783, 222, 49, 42, 47, 222, 1410, 333, 222, 1163, 2977, 15, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "labels[\"input_ids\"][0]: [50373, 311, 222, 426, 437, 222, 8336, 222, 2541, 291, 222, 21609, 222, 11770, 437, 222, 6308, 291, 222, 11084, 222, 2880, 15, 222, 50369, 274, 222, 3241, 333, 222, 11084, 222, 4656, 222, 6308, 381, 222, 3062, 333, 222, 11157, 15, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "labels[\"input_ids\"][0]: [16544, 311, 222, 3443, 333, 222, 6077, 2559, 222, 588, 15, 222, 50373, 311, 222, 337, 655, 222, 1702, 291, 222, 1210, 443, 222, 756, 291, 222, 297, 957, 222, 1990, 588, 15, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "labels[\"input_ids\"][0]: [50369, 274, 222, 50373, 655, 222, 25, 629, 222, 483, 667, 222, 2111, 333, 222, 4213, 701, 222, 891, 1987, 222, 20982, 13, 222, 5284, 222, 452, 2511, 222, 985, 222, 13901, 222, 636, 588, 15, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "labels[\"input_ids\"][0]: [50373, 311, 222, 50373, 262, 222, 2705, 222, 3723, 279, 222, 2128, 278, 222, 6164, 222, 781, 279, 222, 12136, 389, 222, 804, 222, 3723, 333, 222, 5239, 222, 1846, 550, 15, 222, 50373, 381, 222, 50369, 274, 222, 1395, 293, 222, 1400, 429, 222, 398, 381, 222, 635, 3708, 222, 1631, 291, 222, 862, 2758, 279, 222, 1162, 222, 3561, 550, 15, 222, 50373, 311, 222, 1021, 222, 341, 279, 222, 3075, 291, 222, 863, 222, 2056, 293, 222, 17854, 433, 222, 550, 15, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "labels[\"input_ids\"][0]: [50373, 311, 222, 8868, 655, 222, 497, 11911, 291, 222, 1282, 1235, 222, 19773, 333, 222, 12001, 222, 1085, 222, 3450, 291, 222, 49068, 15, 222, 50373, 311, 222, 1230, 222, 804, 222, 457, 352, 302, 222, 1216, 291, 222, 1531, 588, 15, 222, 890, 222, 8868, 274, 222, 1639, 262, 222, 6607, 2345, 222, 636, 443, 222, 12092, 6094, 291, 222, 3312, 222, 2858, 15, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "labels[\"input_ids\"][0]: [548, 1243, 222, 50373, 655, 222, 568, 279, 222, 43568, 977, 262, 222, 1024, 222, 491, 222, 337, 302, 222, 1385, 222, 3191, 302, 222, 1348, 302, 222, 1025, 2911, 222, 1151, 262, 222, 5849, 1024, 222, 1649, 222, 4928, 15, 222, 50373, 311, 222, 987, 274, 222, 3450, 291, 222, 34516, 222, 411, 222, 483, 706, 222, 3535, 588, 15, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "labels[\"input_ids\"][0]: [50373, 311, 222, 3217, 13999, 222, 10904, 222, 50369, 222, 21486, 279, 222, 36083, 222, 1024, 13, 222, 337, 5538, 222, 3217, 13999, 222, 5354, 222, 398, 381, 222, 3217, 294, 302, 222, 20285, 381, 222, 15438, 279, 222, 1162, 222, 1170, 333, 222, 2032, 10342, 15, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "labels[\"input_ids\"][0]: [50373, 262, 222, 576, 765, 279, 222, 18, 13, 20659, 5366, 291, 222, 53, 222, 16, 222, 53, 293, 222, 6173, 1879, 222, 1566, 222, 50369, 311, 222, 33934, 278, 222, 1748, 5366, 893, 222, 9522, 15, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc11429ae0e2481e936b10395c655e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels[\"input_ids\"][0]: [50369, 274, 222, 1664, 1040, 3945, 222, 3946, 291, 222, 3112, 2234, 15, 222, 2183, 274, 222, 50373, 655, 222, 7090, 222, 1162, 222, 10732, 13, 222, 50369, 333, 222, 1393, 222, 6618, 655, 222, 5234, 222, 1232, 570, 15, 222, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "    \n",
    "    # 대화 내용을 프롬프트와 결합하여 입력 생성\n",
    "    inputs = [\"summarize: \" for item in sample[\"dialogue\"]]\n",
    "\n",
    "    # max_length에 프롬프트의 토큰 수를 고려한 길이를 설정\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length + 20, padding=padding, truncation=True, add_special_tokens=True)\n",
    "\n",
    "# 타겟(summary_en)도 토큰화\n",
    "    labels = tokenizer(text_target=sample[\"summary\"], \n",
    "                        max_length=max_target_length, \n",
    "                        padding=padding, \n",
    "                        truncation=True)\n",
    "    \n",
    "    \n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        \n",
    "        # if isinstance(labels[\"input_ids\"][0], list):  # Check if it is a list of lists\n",
    "        #     print(f'labels[\"input_ids\"][0]: {labels[\"input_ids\"][0]}')\n",
    "        #     labels[\"input_ids\"] = [\n",
    "        #         [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        #     ]\n",
    "        # else:  # Handle single instance case\n",
    "        #     print(f'labels[\"input_ids\"]: {labels[\"input_ids\"]}')\n",
    "        #     labels[\"input_ids\"] = [(l if l != tokenizer.pad_token_id else -100) for l in labels[\"input_ids\"]]\n",
    "        print(f'labels[\"input_ids\"][0]: {labels[\"input_ids\"][0]}')\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    # model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    # return model_inputs\n",
    "    \n",
    "    # with tokenizer.as_target_tokenizer():\n",
    "    #     labels = tokenizer(sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# 데이터셋에 전처리 함수를 적용\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=['fname', 'dialogue', 'summary', 'topic','dialogue_en', 'summary_en', 'topic_en'])\n",
    "\n",
    "# 처리된 데이터셋의 키 출력\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /data/ephemeral/home/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# helper function to postprocess text\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    # 정수 배열로 변환하고, 범위를 tokenizer의 vocab 크기로 제한\n",
    "    preds = np.array(preds, dtype=np.int64)\n",
    "    preds = np.clip(preds, 0, tokenizer.vocab_size - 1)\n",
    "    print(preds)\n",
    "  \n",
    "    # 토큰 ID를 텍스트로 디코딩\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    print(decoded_preds)\n",
    "    # 라벨에서 -100을 패딩 토큰 ID로 대체\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # 메트릭 계산 후 결과 반환\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    \n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration,T5TokenizerFast\n",
    "\n",
    "# load model from the hub\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_id)\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# load model from the hub\n",
    "#model = T5TokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3856150448.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[25], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    wandb login\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "wandb login\n",
    "# 37ef351873d76557e00679959886f35cb3bbc35c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1578' max='15560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1578/15560 1:28:55 < 13:08:57, 0.30 it/s, Epoch 2.03/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.409900</td>\n",
       "      <td>2.703831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.674500</td>\n",
       "      <td>2.498986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0 50257   262 ...   550    15     1]\n",
      " [    0 50257   262 ...   550    15     1]\n",
      " [    0 50257   262 ...   550    15     1]\n",
      " ...\n",
      " [    0 50257   262 ...   550    15     1]\n",
      " [    0 50257   262 ...   550    15     1]\n",
      " [    0 50257   262 ...   550    15     1]]\n",
      "['사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.', '사시는데이 사람은 그의 아버지가 그의 아버지에게 그의 아버지가 그에게 돈을 빌려 달라고 요청합니다.']\n",
      "[[    0 50257   274 ...   588    15     1]\n",
      " [    0 50257   274 ...   588    15     1]\n",
      " [    0 50257   274 ...   588    15     1]\n",
      " ...\n",
      " [    0 50257   274 ...   588    15     1]\n",
      " [    0 50257   274 ...   588    15     1]\n",
      " [    0 50257   274 ...   588    15     1]]\n",
      "['사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.', '사시는데는 자신의 집에 대해 이야기하고 있다. 그는 자신의 집에 대해 이야기한다.']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Training 시작\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3349\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2192\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2193\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from huggingface_hub import HfFolder\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "# GPU 사용 가능 여부 확인 및 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Hugging Face repository id\n",
    "repository_id = f\"{model_id.split('/')[1]}-{dataset_id}\"\n",
    "\n",
    "# Define training args with additional parameters\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    num_train_epochs=20,  # 총 20 에폭 동안 학습\n",
    "    learning_rate=1e-5,  # 학습률\n",
    "    per_device_train_batch_size=1,  # 훈련 중 한 장치당 배치 크기\n",
    "    per_device_eval_batch_size=1,  # 평가 중 한 장치당 배치 크기\n",
    "    warmup_ratio=0.1,  # 워밍업 비율\n",
    "    weight_decay=0.01,  # 가중치 감쇠\n",
    "    lr_scheduler_type='cosine',  # 코사인 스케줄러 사용\n",
    "    optim='adamw_torch',  # 옵티마이저: AdamW 사용\n",
    "    gradient_accumulation_steps=16,  # 기울기 누적 단계\n",
    "    evaluation_strategy='epoch',  # 에폭 단위로 평가\n",
    "    save_strategy='epoch',  # 에폭 단위로 저장\n",
    "    save_total_limit=5,  # 총 5개의 체크포인트를 저장\n",
    "    fp16=True,  # mixed precision 학습 활성화 # True로 하면 overflow\n",
    "    load_best_model_at_end=True,  # 가장 좋은 모델을 마지막에 로드\n",
    "    seed=42,  # 재현성을 위한 시드 값\n",
    "    logging_dir=\"./logs\",  # 로그 디렉토리\n",
    "    logging_strategy=\"epoch\",  # 에폭마다 로깅\n",
    "    predict_with_generate=True,  # 생성 모드를 사용할 때 평가 설정\n",
    "    generation_max_length=max_target_length,  # 최대 생성 길이\n",
    "    do_train=True,  # 학습 여부\n",
    "    do_eval=True,  # 평가 여부\n",
    ")\n",
    "\n",
    "# Create Trainer instance with early stopping callback\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"val\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(\n",
    "            early_stopping_patience=3,  # 3번의 에폭 동안 개선되지 않으면 중단\n",
    "            early_stopping_threshold=0.001  # 성능이 0.001만큼 개선되지 않으면 중단\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# GPU 메모리 캐시를 지웁니다.\n",
    "torch.cuda.empty_cache()\n",
    "# Training 시작\n",
    "trainer.train()\n",
    "# 20 epoch: 13시간 예상\n",
    "# 1 epoch: 0.69시간 예상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 819\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='103' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [103/103 01:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.3715944290161133,\n",
       " 'eval_rouge1': 47.2358,\n",
       " 'eval_rouge2': 23.5135,\n",
       " 'eval_rougeL': 39.6266,\n",
       " 'eval_rougeLsum': 43.3458,\n",
       " 'eval_gen_len': 17.39072039072039,\n",
       " 'eval_runtime': 108.99,\n",
       " 'eval_samples_per_second': 7.514,\n",
       " 'eval_steps_per_second': 0.945,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score we achieved is an `rouge1` score of `47.23`. \n",
    "\n",
    "Lets save our results and tokenizer to the Hugging Face Hub and create a model card. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our tokenizer and create model card\n",
    "tokenizer.save_pretrained(repository_id)\n",
    "trainer.create_model_card()\n",
    "# Push the results to the hub\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Inference\n",
    "\n",
    "Now we have a trained model, we can use it to run inference. We will use the `pipeline` API from transformers and a `test` example from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialogue: \n",
      "Abby: Have you talked to Miro?\n",
      "Dylan: No, not really, I've never had an opportunity\n",
      "Brandon: me neither, but he seems a nice guy\n",
      "Brenda: you met him yesterday at the party?\n",
      "Abby: yes, he's so interesting\n",
      "Abby: told me the story of his father coming from Albania to the US in the early 1990s\n",
      "Dylan: really, I had no idea he is Albanian\n",
      "Abby: he is, he speaks only Albanian with his parents\n",
      "Dylan: fascinating, where does he come from in Albania?\n",
      "Abby: from the seacoast\n",
      "Abby: Duress I believe, he told me they are not from Tirana\n",
      "Dylan: what else did he tell you?\n",
      "Abby: That they left kind of illegally\n",
      "Abby: it was a big mess and extreme poverty everywhere\n",
      "Abby: then suddenly the border was open and they just left \n",
      "Abby: people were boarding available ships, whatever, just to get out of there\n",
      "Abby: he showed me some pictures, like <file_photo>\n",
      "Dylan: insane\n",
      "Abby: yes, and his father was among the people\n",
      "Dylan: scary but interesting\n",
      "Abby: very!\n",
      "---------------\n",
      "flan-t5-base summary:\n",
      "Abby met Miro yesterday at the party. Miro's father came from Albania to the US in the early 1990s. He speaks Albanian with his parents. The border was open and people were boarding ships to get out of there.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from random import randrange        \n",
    "\n",
    "# load model and tokenizer from huggingface hub with pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"philschmid/flan-t5-base-samsum\", device=0)\n",
    "\n",
    "# select a random test sample\n",
    "sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n",
    "print(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n",
    "\n",
    "# summarize dialogue\n",
    "res = summarizer(sample[\"dialogue\"])\n",
    "\n",
    "print(f\"flan-t5-base summary:\\n{res[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
