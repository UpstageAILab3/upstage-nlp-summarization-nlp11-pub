{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/meta-llama/llama-recipes/blob/main/recipes/finetuning/quickstart_peft_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT Finetuning Quick Start Notebook\n",
    "\n",
    "This notebook shows how to train a Meta Llama 3 model on a single GPU (e.g. A10 with 24GB) using int8 quantization and LoRA finetuning.\n",
    "\n",
    "**_Note:_** To run this notebook on a machine with less than 24GB VRAM (e.g. T4 with 16GB) the context length of the training dataset needs to be adapted.\n",
    "We do this based on the available VRAM during execution.\n",
    "If you run into OOM issues try to further lower the value of train_config.context_length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Install pre-requirements and convert checkpoint\n",
    "\n",
    "We need to have llama-recipes and its dependencies installed for this notebook. Additionally, we need to log in with the huggingface_cli and make sure that the account is able to to access the Meta Llama weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if running from Colab T4\n",
    "# ! pip install llama-recipes ipywidgets\n",
    "\n",
    "# import huggingface_hub\n",
    "# huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939288fac82f463dacb3b1922cb7dc03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login()\n",
    "# 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"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the model\n",
    "\n",
    "Setup training configuration and load the model and tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/opt/conda/lib/python3.10/site-packages/llama_recipes/configs/datasets.py 에 셋팅함\n",
    "@dataclass\n",
    "class upstage_dataset:\n",
    "    dataset: str = \"upstage_dataset\"\n",
    "    file: str = \"/data/ephemeral/home/upstage-nlp-summarization-nlp11/llama_recipes/my_datasets/custom_dataset.py\"\n",
    "    train_file: str = \"/data/ephemeral/home/data/train.csv\" \n",
    "    validation_file: str = \"/data/ephemeral/home/data/dev.csv\"\n",
    "    train_split: str = \"train\"\n",
    "    test_split: str = \"validation\"\n",
    "\n",
    "/opt/conda/lib/python3.10/site-packages/llama_recipes/utils/dataset_utils.py 수정\n",
    "from llama_recipes.datasets import (\n",
    "    get_grammar_dataset,\n",
    "    get_alpaca_dataset,\n",
    "    get_samsum_dataset,\n",
    "    get_llamaguard_toxicchat_dataset,\n",
    "    get_upstage_dataset,\n",
    ")\n",
    "DATASET_PREPROC = {\n",
    "    \"alpaca_dataset\": partial(get_alpaca_dataset),\n",
    "    \"grammar_dataset\": get_grammar_dataset,\n",
    "    \"samsum_dataset\": get_samsum_dataset,\n",
    "    \"custom_dataset\": get_custom_dataset,\n",
    "    \"llamaguard_toxicchat_dataset\": get_llamaguard_toxicchat_dataset,\n",
    "    \"upstage_dataset\": get_upstage_dataset,\n",
    "}\n",
    "\n",
    "/opt/conda/lib/python3.10/site-packages/llama_recipes/datasets/__init__.py 수정\n",
    "from llama_recipes.datasets.upstage_dataset import get_preprocessed_upstage as get_upstage_dataset # upstage_dataset 추가\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#/opt/conda/lib/python3.10/site-packages/llama_recipes/datasets/upstage_dataset.py 파일 생성\n",
    "\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n",
    "\n",
    "\n",
    "import copy\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "def get_preprocessed_upstage(dataset_config, tokenizer, split):\n",
    "    # dataset = datasets.load_dataset(\"samsum\", split=split)\n",
    "    print(\"******************/opt/conda/lib/python3.10/site-packages/llama_recipes/datasets/upstage_dataset.py*************************\")\n",
    "   \n",
    "    data_files = {\n",
    "        \"train\": dataset_config.train_file,\n",
    "        \"validation\": dataset_config.validation_file\n",
    "    }\n",
    "    dataset = load_dataset(\"csv\", data_files={split: data_files[split]}, split=split)\n",
    "\n",
    "    prompt = (\n",
    "        f\"Summarize this dialog:\\n{{dialog}}\\n---\\nSummary:\\n\"\n",
    "    )\n",
    "\n",
    "    def apply_prompt_template(sample):\n",
    "        return {\n",
    "            \"prompt\": prompt.format(dialog=sample[\"dialogue\"]),\n",
    "            \"summary\": sample[\"summary\"],\n",
    "        }\n",
    "\n",
    "    dataset = dataset.map(apply_prompt_template, remove_columns=list(dataset.features))\n",
    "\n",
    "    def tokenize_add_label(sample):\n",
    "        prompt = tokenizer.encode(tokenizer.bos_token + sample[\"prompt\"], add_special_tokens=False)\n",
    "        summary = tokenizer.encode(sample[\"summary\"] +  tokenizer.eos_token, add_special_tokens=False)\n",
    "\n",
    "        sample = {\n",
    "            \"input_ids\": prompt + summary,\n",
    "            \"attention_mask\" : [1] * (len(prompt) + len(summary)),\n",
    "            \"labels\": [-100] * len(prompt) + summary,\n",
    "            }\n",
    "\n",
    "        return sample\n",
    "\n",
    "    dataset = dataset.map(tokenize_add_label, remove_columns=list(dataset.features))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f8cfd524ad46b984245b90214998d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from llama_recipes.configs import train_config as TRAIN_CONFIG\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "\n",
    "train_config = TRAIN_CONFIG()\n",
    "train_config.model_name = model_name\n",
    "train_config.num_epochs = 1 \n",
    "train_config.run_validation = False\n",
    "train_config.gradient_accumulation_steps = 4\n",
    "train_config.num_workers_dataloader = 4\n",
    "train_config.batch_size_training = 1\n",
    "train_config.lr = 3e-4\n",
    "train_config.use_fast_kernels = True\n",
    "train_config.use_fp16 = True\n",
    "train_config.context_length = 1024 if torch.cuda.get_device_properties(0).total_memory < 16e9 else 2048 # T4 16GB or A10 24GB\n",
    "train_config.batching_strategy = \"packing\"\n",
    "train_config.output_dir = model_name\n",
    "# train_config.use_peft = True\n",
    "# train_config.save_strategy=\"epoch\"  # 매 에포크마다 저장\n",
    "# train_config.load_best_model_at_end=True  # 가장 성능 좋은 모델을 훈련 종료 시 로드\n",
    "# train_config.metric_for_best_model=\"accuracy\"  # 최적 모델을 선택할 기준\n",
    "# train_config.greater_is_better=True,  # 성능 지표가 높을수록 좋다고 가정\n",
    "\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    #np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "train_config.dataset = \"upstage_dataset\" \n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "            train_config.model_name,\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=config,\n",
    "            use_cache=False,\n",
    "            attn_implementation=\"sdpa\" if train_config.use_fast_kernels else None,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(train_config.model_name, clean_up_tokenization_spaces=True) #공백처리\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer.pad_token = tokenizer.eos_token`로 설정하는 이유는 주로 **모델이 `pad_token`을 따로 가지고 있지 않은 경우**에 발생하는 문제를 해결하기 위해서입니다. 이 설정을 통해 **`pad_token`과 `eos_token`(문장의 끝을 나타내는 토큰)**을 동일하게 취급하게 됩니다.\n",
    "\n",
    "### 이유:\n",
    "1. **Llama 모델 구조**:\n",
    "   - Llama와 같은 일부 모델들은 기본적으로 `pad_token`을 따로 정의하지 않고, 주로 **`eos_token`**만을 사용합니다. \n",
    "   - 이런 경우, 시퀀스의 길이가 고정되지 않은 상태에서, 미리 정의된 `pad_token`이 없으면, 모델에 데이터를 전달할 때 문제(에러)가 발생할 수 있습니다.\n",
    "\n",
    "2. **패딩이 필요한 상황**:\n",
    "   - **배치 학습**에서 시퀀스 길이가 다르면 패딩을 넣어서 길이를 맞춰야 합니다. \n",
    "   - 하지만 Llama와 같은 모델에는 `pad_token`이 없기 때문에 **패딩이 필요할 때 `eos_token`을 대신 사용하는** 것입니다.\n",
    "   - `pad_token`이 필요 없는 문장에서 `eos_token`은 문장의 끝을 나타내기 때문에, `eos_token`으로 패딩을 해도 의미상 큰 문제가 없습니다.\n",
    "\n",
    "3. **`pad_token`과 `eos_token`의 차이**:\n",
    "   - **`eos_token`**: 문장이 끝났음을 나타냅니다. 텍스트 생성을 중단할 시점입니다.\n",
    "   - **`pad_token`**: 고정된 시퀀스 길이에 맞춰 텍스트가 부족한 부분을 채우기 위한 용도입니다.\n",
    "   - 두 토큰의 목적은 다르지만, **모델이 따로 `pad_token`을 갖고 있지 않은 경우**, `eos_token`을 대신 사용하여 패딩을 처리할 수 있습니다.\n",
    "\n",
    "결론적으로, 이 코드는 Llama 모델처럼 `pad_token`이 없는 모델에서 **패딩과 문장 끝의 처리를 동일하게 하기 위해** 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep  9 04:08:39 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  | 00000000:4C:00.0 Off |                  N/A |\n",
      "| 40%   41C    P2             104W / 350W |   9010MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Check base model\n",
    "\n",
    "Run the base model on an example input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?\\n#Person2#: 건강검진을 받는 것이 좋을 것 같아서요.\\n#Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 않았습니다. 매년 받아야 합니다.\\n#Person2#: 알고 있습니다. 하지만 아무 문제가 없다면 왜 의사를 만나러 가야 하나요?\\n#Person1#: 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 오세요.\\n#Person2#: 알겠습니다.\\n#Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 피우시나요?\\n#Person2#: 네.\\n#Person1#: 당신도 알다시피, 담배는 폐암과 심장병의 주요 원인입니다. 정말로 끊으셔야 합니다. \\n#Person2#: 수백 번 시도했지만, 습관을 버리는 것이 어렵습니다.\\n#Person1#: 우리는 도움이 될 수 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다.\\n#Person2#: 알겠습니다, 감사합니다, 의사선생님.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "dataset = load_dataset('csv', data_files={'train': \"/data/ephemeral/home/data/train_10.csv\", 'val': \"/data/ephemeral/home/data/dev.csv\"})\n",
    "\n",
    "dataset['train']['dialogue'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "다음 대화를 요약하세요:\n",
      "#Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?\n",
      "#Person2#: 건강검진을 받는 것이 좋을 것 같아서요.\n",
      "#Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 않았습니다. 매년 받아야 합니다.\n",
      "#Person2#: 알고 있습니다. 하지만 아무 문제가 없다면 왜 의사를 만나러 가야 하나요?\n",
      "#Person1#: 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 오세요.\n",
      "#Person2#: 알겠습니다.\n",
      "#Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 피우시나요?\n",
      "#Person2#: 네.\n",
      "#Person1#: 당신도 알다시피, 담배는 폐암과 심장병의 주요 원인입니다. 정말로 끊으셔야 합니다. \n",
      "#Person2#: 수백 번 시도했지만, 습관을 버리는 것이 어렵습니다.\n",
      "#Person1#: 우리는 도움이 될 수 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다.\n",
      "#Person2#: 알겠습니다, 감사합니다, 의사선생님.\n",
      "---\n",
      "요약:\n",
      "1. 호킨스 의사는 스미스씨에게 건강검진을 받는 것이 좋을 것 같다고 말한다.\n",
      "2. 스미스씨는 건강검진을 받는 것이 좋다고 생각하지만, 아무 문제가 없다면 왜 의사를 만나러 가야 하는지 의문을 제기한다.\n",
      "3. 호킨스 의사는 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것이라고 말한다.\n",
      "4. 스미스\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"\n",
    "다음 대화를 요약하세요:\n",
    "#Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?\\n#Person2#: 건강검진을 받는 것이 좋을 것 같아서요.\\n#Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 않았습니다. 매년 받아야 합니다.\\n#Person2#: 알고 있습니다. 하지만 아무 문제가 없다면 왜 의사를 만나러 가야 하나요?\\n#Person1#: 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 오세요.\\n#Person2#: 알겠습니다.\\n#Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 피우시나요?\\n#Person2#: 네.\\n#Person1#: 당신도 알다시피, 담배는 폐암과 심장병의 주요 원인입니다. 정말로 끊으셔야 합니다. \\n#Person2#: 수백 번 시도했지만, 습관을 버리는 것이 어렵습니다.\\n#Person1#: 우리는 도움이 될 수 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다.\\n#Person2#: 알겠습니다, 감사합니다, 의사선생님.\n",
    "---\n",
    "요약:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the base model only repeats the conversation.\n",
    "\n",
    "### Step 3: Load the preprocessed dataset \n",
    "\n",
    "We load and preprocess the samsum dataset which consists of curated pairs of dialogs and their summarization:\n",
    "\n",
    "https://github.com/meta-llama/llama-recipes/blob/main/recipes/quickstart/finetuning/datasets/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpaca_dataset': functools.partial(<class 'llama_recipes.datasets.alpaca_dataset.InstructionDataset'>), 'grammar_dataset': <function get_dataset at 0x7faae09b7250>, 'samsum_dataset': <function get_preprocessed_samsum at 0x7fab56afecb0>, 'custom_dataset': <function get_custom_dataset at 0x7faae09b71c0>, 'llamaguard_toxicchat_dataset': <function get_llamaguard_toxicchat_dataset at 0x7fab56989ea0>, 'upstage_dataset': <function get_preprocessed_upstage at 0x7fab5698a170>}\n",
      "('alpaca_dataset', 'grammar_dataset', 'samsum_dataset', 'custom_dataset', 'llamaguard_toxicchat_dataset', 'upstage_dataset')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n",
      "  from torch.distributed._shard.checkpoint import (\n"
     ]
    }
   ],
   "source": [
    "from llama_recipes.utils.config_utils import generate_dataset_config\n",
    "\n",
    "dataset_config = generate_dataset_config(train_config, {})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "upstage_dataset(dataset='upstage_dataset', file='/data/ephemeral/home/upstage-nlp-summarization-nlp11/llama_recipes/my_datasets/custom_dataset.py', train_file='/data/ephemeral/home/data/train_dev.csv', validation_file='/data/ephemeral/home/data/dev.csv', train_split='train', test_split='validation')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************/opt/conda/lib/python3.10/site-packages/llama_recipes/datasets/upstage_dataset.py*************************\n",
      "******************/opt/conda/lib/python3.10/site-packages/llama_recipes/datasets/upstage_dataset.py*************************\n"
     ]
    }
   ],
   "source": [
    "from llama_recipes.utils.dataset_utils import get_preprocessed_dataset\n",
    "train_dataset = get_preprocessed_dataset(tokenizer, dataset_config, split=\"train\")\n",
    "val_dataset = get_preprocessed_dataset(tokenizer, dataset_config, split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 12956\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 499\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([143])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(train_dataset[\"input_ids\"][5]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([143])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(train_dataset[\"labels\"][5]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset: 100%|██████████| 12956/12956 [00:03<00:00, 3386.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from llama_recipes.data.concatenator import ConcatDataset\n",
    "from llama_recipes.utils.config_utils import get_dataloader_kwargs\n",
    "\n",
    "train_dl_kwargs = get_dataloader_kwargs(train_config, train_dataset, tokenizer, \"train\")\n",
    "\n",
    "if train_config.batching_strategy == \"packing\":\n",
    "        train_dataset = ConcatDataset(train_dataset, chunk_size=train_config.context_length)\n",
    "\n",
    "# Create DataLoaders for the training and validation dataset\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    num_workers=train_config.num_workers_dataloader,\n",
    "    pin_memory=True,\n",
    "    **train_dl_kwargs,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fac3141c0a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Prepare model for PEFT\n",
    "\n",
    "Let's prepare the model for Parameter Efficient Fine Tuning (PEFT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, prepare_model_for_kbit_training, LoraConfig\n",
    "from dataclasses import asdict\n",
    "from llama_recipes.configs import lora_config as LORA_CONFIG\n",
    "\n",
    "lora_config = LORA_CONFIG()\n",
    "lora_config.r = 32\n",
    "lora_config.lora_alpha = 32\n",
    "lora_dropout: float=0.1\n",
    "\n",
    "# lora_config = LORA_CONFIG()\n",
    "# lora_config.r = 32\n",
    "# lora_config.lora_alpha = 32\n",
    "# lora_config.lora_dropout = 0.05\n",
    "# lora_config.target_modules = [\"q_proj\", \"v_proj\"]  # LoRA를 적용할 모듈\n",
    "# lora_config.merge_weights = False  # 학습 후 가중치 병합 여부\n",
    "# lora_config.use_scaled_init = True  # 스케일된 가중치 초기화\n",
    "# lora_config.fan_in_fan_out = False  # fan-in/fan-out 구조 고려 여부\n",
    "# lora_config.bias = \"none\"  # Bias 적용 여부\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(**asdict(lora_config))\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Fine tune the model\n",
    "\n",
    "Here, we fine tune the model for a single epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/llama_recipes/utils/train_utils.py:92: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Training Epoch: 1:   0%|\u001b[34m          \u001b[0m| 0/465 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/llama_recipes/utils/train_utils.py:151: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Training Epoch: 1/1, step 1862/1863 completed (loss: 0.18174602091312408): : 466it [1:29:42, 11.55s/it]                       1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max CUDA memory allocated was 15 GB\n",
      "Max CUDA memory reserved was 17 GB\n",
      "Peak active CUDA memory was 15 GB\n",
      "CUDA Malloc retries : 0\n",
      "CPU Total Peak Memory consumed during the train (max): 2 GB\n",
      "Epoch 1: train_perplexity=1.2732, train_epoch_loss=0.2415, epoch time 5382.916620232165s\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from llama_recipes.utils.train_utils import train\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=train_config.lr,\n",
    "            weight_decay=train_config.weight_decay,\n",
    "        )\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=train_config.gamma)\n",
    "\n",
    "# Start the training process\n",
    "results = train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    None,\n",
    "    tokenizer,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    train_config.gradient_accumulation_steps,\n",
    "    train_config,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    wandb_run=None,\n",
    ")\n",
    "\n",
    "# 1시 10분에 시작."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm  # 진행창을 위한 tqdm 추가\n",
    "from llama_recipes.utils.train_utils import train\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# 수정된 train 함수 (OverflowError: out of range integral type conversion attempted 에러: 검증 루프 제거)\n",
    "def train_model(model, train_dataloader, tokenizer, optimizer, scheduler, train_config,\n",
    "                gradient_accumulation_steps, fsdp_config, local_rank, rank, wandb_run):\n",
    "    \n",
    "    os.makedirs(train_config.output_dir, exist_ok=True)\n",
    "    \n",
    "    for epoch in range(train_config.num_epochs):\n",
    "        # 모델을 훈련 모드로 설정\n",
    "        model.train()\n",
    "\n",
    "        # 훈련 루프 (tqdm을 사용해 진행창 표시)\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\")):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 텐서로 변환\n",
    "            batch = {k: v.to('cuda:0') for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Gradient Accumulation 적용\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient Accumulation을 고려하여 optimizer step\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "        # 모델 저장\n",
    "        torch.save(model.state_dict(), f\"{train_config.output_dir}/model_epoch_{epoch+1}.pt\")\n",
    "        print(f\"Model saved after epoch {epoch+1}\")\n",
    "\n",
    "        # Optionally, log metrics to Weights & Biases\n",
    "        if wandb_run is not None:\n",
    "            wandb_run.log({\"epoch\": epoch + 1})\n",
    "\n",
    "# Optimizer 및 Scheduler 설정\n",
    "optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=train_config.lr,\n",
    "            weight_decay=train_config.weight_decay,\n",
    "        )\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=train_config.gamma)\n",
    "\n",
    "# 수정된 함수 호출 (검증 루프 제거)\n",
    "results = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    train_config=train_config,\n",
    "    gradient_accumulation_steps=train_config.gradient_accumulation_steps,  # Gradient Accumulation 적용\n",
    "    fsdp_config=None,  # 필요한 경우 적용, 현재는 None # 분산학습\n",
    "    local_rank=None,  # 필요한 경우 적용, 현재는 None\n",
    "    rank=None,  # 필요한 경우 적용, 현재는 None\n",
    "    wandb_run=None  # Weights & Biases로 로깅할 경우 설정\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6:\n",
    "Save model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type Llama to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('meta-llama/Meta-Llama-3.1-8B/tokenizer_config.json',\n",
       " 'meta-llama/Meta-Llama-3.1-8B/special_tokens_map.json',\n",
       " 'meta-llama/Meta-Llama-3.1-8B/tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# 모델 저장 경로\n",
    "output_dir = train_config.output_dir\n",
    "\n",
    "# config.json 파일이 없는 경우 수동으로 작성 (OSError: beomi/Llama-3-Open-Ko-8B does not appear to have a file named config.json 에러)\n",
    "config = {\n",
    "    \"model_type\": \"Llama\",  # 모델 타입\n",
    "    \"vocab_size\": model.config.vocab_size,  # 필요한 경우 다른 속성도 추가\n",
    "    \"hidden_size\": model.config.hidden_size,\n",
    "    \"num_attention_heads\": model.config.num_attention_heads,\n",
    "    # 추가적인 필요한 구성 옵션들\n",
    "}\n",
    "\n",
    "# 디렉토리 생성\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# config.json 저장\n",
    "with open(os.path.join(output_dir, \"config.json\"), \"w\") as f:\n",
    "    json.dump(config, f)\n",
    "\n",
    "# 모델과 토크나이저 저장\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meta-llama/Meta-Llama-3.1-8B'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a7c544471db4cc58f39e5d66b498dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/SummerHotBreeze/Meta-Llama-3.1-8B/commit/2eadde4207f520d1d33f903a5fdd929e57c2f9fc', commit_message='Upload tokenizer', commit_description='', oid='2eadde4207f520d1d33f903a5fdd929e57c2f9fc', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.push_to_hub(f\"SummerHotBreeze/Meta-Llama-3.1-8B\")\n",
    "tokenizer.push_to_hub(f\"SummerHotBreeze/Meta-Llama-3.1-8B\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7:\n",
    "Try the fine tuned model on the same example again to see the learning progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class DatasetForLlamaTest(Dataset):\n",
    "    def __init__(self, train_csv_fullpath):\n",
    "        df = pd.read_csv(train_csv_fullpath)\n",
    "        \n",
    "        prompt = (\n",
    "            f\"Summarize this dialog:\\n{{dialog}}\\n---\\nSummary:\\n\"\n",
    "        )\n",
    "\n",
    "        def apply_prompt_template(s):\n",
    "            return prompt.format(dialog=s)\n",
    "\n",
    "        df['dialogue'] = df['dialogue'].map(apply_prompt_template)\n",
    "\n",
    "        self.fname_list = []\n",
    "        self.preprocessed_list = []\n",
    "        \n",
    "        for i in range(len(df)):\n",
    "            #\n",
    "            self.fname_list.append(df.iloc[i]['fname'])\n",
    "            \n",
    "            # tokenizer() 와 tokenizer.encode() 는 다르다!\n",
    "            # tokenizer() 를 사용해야 딕셔너리(input_ids, attention_mask) 형태로 리턴됨.\n",
    "            prompt = tokenizer(df.iloc[i]['dialogue'], return_tensors=\"pt\").to(\"cuda\")\n",
    "            self.preprocessed_list.append(prompt)\n",
    "        \n",
    "        self.len = len(self.preprocessed_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.fname_list[idx], self.preprocessed_list[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/499 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  0%|          | 1/499 [00:25<3:30:05, 25.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  0%|          | 2/499 [00:36<2:21:02, 17.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|          | 3/499 [00:42<1:38:29, 11.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|          | 4/499 [00:56<1:44:05, 12.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|          | 5/499 [01:13<1:57:35, 14.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|          | 6/499 [01:33<2:12:35, 16.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|▏         | 7/499 [01:40<1:47:51, 13.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▏         | 8/499 [01:52<1:46:17, 12.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▏         | 9/499 [02:02<1:36:45, 11.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▏         | 10/499 [02:07<1:20:21,  9.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▏         | 11/499 [02:15<1:14:49,  9.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▏         | 12/499 [02:25<1:18:00,  9.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|▎         | 13/499 [02:34<1:15:32,  9.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|▎         | 14/499 [02:41<1:11:13,  8.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|▎         | 15/499 [02:53<1:17:43,  9.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|▎         | 16/499 [03:05<1:23:24, 10.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|▎         | 17/499 [03:10<1:10:37,  8.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  4%|▎         | 18/499 [03:16<1:04:25,  8.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  4%|▍         | 19/499 [03:20<53:26,  6.68s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  4%|▍         | 20/499 [03:28<56:28,  7.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  4%|▍         | 21/499 [03:40<1:08:45,  8.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  4%|▍         | 22/499 [03:51<1:12:34,  9.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|▍         | 23/499 [03:59<1:11:51,  9.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|▍         | 24/499 [04:05<1:04:31,  8.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|▌         | 25/499 [04:12<1:00:59,  7.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|▌         | 26/499 [04:24<1:10:55,  9.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|▌         | 27/499 [04:31<1:04:40,  8.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|▌         | 28/499 [04:52<1:36:45, 12.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|▌         | 29/499 [05:02<1:29:04, 11.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|▌         | 30/499 [05:04<1:08:32,  8.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|▌         | 31/499 [05:13<1:07:13,  8.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|▋         | 32/499 [05:17<58:24,  7.50s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 33/499 [05:25<58:04,  7.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 34/499 [05:35<1:03:50,  8.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 35/499 [05:44<1:06:44,  8.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 36/499 [05:54<1:08:29,  8.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 37/499 [06:10<1:26:04, 11.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|▊         | 38/499 [06:19<1:19:25, 10.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|▊         | 39/499 [06:31<1:23:40, 10.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|▊         | 40/499 [06:41<1:20:35, 10.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|▊         | 41/499 [06:51<1:19:34, 10.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|▊         | 42/499 [07:12<1:43:24, 13.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|▊         | 43/499 [07:21<1:33:39, 12.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|▉         | 44/499 [07:35<1:35:45, 12.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|▉         | 45/499 [07:52<1:46:32, 14.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|▉         | 46/499 [08:11<1:56:52, 15.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|▉         | 47/499 [08:24<1:51:27, 14.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|▉         | 48/499 [08:34<1:41:30, 13.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|▉         | 49/499 [08:43<1:29:56, 11.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|█         | 50/499 [08:54<1:26:59, 11.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|█         | 51/499 [09:02<1:18:33, 10.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|█         | 52/499 [09:10<1:14:21,  9.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|█         | 53/499 [09:17<1:05:58,  8.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|█         | 54/499 [09:23<1:00:22,  8.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|█         | 55/499 [09:31<58:36,  7.92s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|█         | 56/499 [09:35<50:53,  6.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|█▏        | 57/499 [09:57<1:23:43, 11.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█▏        | 58/499 [10:09<1:25:35, 11.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█▏        | 59/499 [10:14<1:10:47,  9.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█▏        | 60/499 [10:36<1:36:30, 13.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█▏        | 61/499 [10:42<1:20:57, 11.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█▏        | 62/499 [10:51<1:17:35, 10.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█▎        | 63/499 [11:09<1:32:42, 12.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█▎        | 64/499 [11:15<1:16:43, 10.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█▎        | 65/499 [11:18<1:01:35,  8.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█▎        | 66/499 [11:24<56:21,  7.81s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█▎        | 67/499 [11:36<1:05:12,  9.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▎        | 68/499 [11:51<1:16:31, 10.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▍        | 69/499 [11:57<1:07:17,  9.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▍        | 70/499 [12:04<1:02:31,  8.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▍        | 71/499 [12:13<1:02:49,  8.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▍        | 72/499 [12:27<1:13:00, 10.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|█▍        | 73/499 [12:39<1:16:38, 10.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|█▍        | 74/499 [12:57<1:32:26, 13.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|█▌        | 75/499 [13:04<1:18:26, 11.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|█▌        | 76/499 [13:16<1:19:28, 11.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|█▌        | 77/499 [13:35<1:36:29, 13.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|█▌        | 78/499 [13:44<1:27:05, 12.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|█▌        | 79/499 [13:54<1:20:20, 11.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|█▌        | 80/499 [14:02<1:13:08, 10.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|█▌        | 81/499 [14:08<1:03:23,  9.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|█▋        | 82/499 [14:12<53:59,  7.77s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|█▋        | 83/499 [14:25<1:03:54,  9.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|█▋        | 84/499 [14:38<1:12:29, 10.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|█▋        | 85/499 [14:44<1:01:08,  8.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|█▋        | 86/499 [14:53<1:02:20,  9.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|█▋        | 87/499 [15:04<1:05:27,  9.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|█▊        | 88/499 [16:23<3:29:06, 30.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|█▊        | 89/499 [16:29<2:38:48, 23.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|█▊        | 90/499 [16:39<2:10:45, 19.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|█▊        | 91/499 [16:43<1:38:24, 14.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|█▊        | 92/499 [16:47<1:17:50, 11.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|█▊        | 93/499 [16:57<1:13:46, 10.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|█▉        | 94/499 [17:06<1:10:12, 10.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|█▉        | 95/499 [17:14<1:04:50,  9.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|█▉        | 96/499 [17:20<56:59,  8.49s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|█▉        | 97/499 [17:24<48:45,  7.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|█▉        | 98/499 [17:34<53:28,  8.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|█▉        | 99/499 [17:45<59:38,  8.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|██        | 100/499 [17:55<1:00:59,  9.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|██        | 101/499 [18:04<1:00:59,  9.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|██        | 102/499 [18:10<55:14,  8.35s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|██        | 103/499 [18:13<44:49,  6.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|██        | 104/499 [18:21<46:25,  7.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|██        | 105/499 [18:29<48:56,  7.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|██        | 106/499 [18:40<55:02,  8.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|██▏       | 107/499 [18:59<1:14:52, 11.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|██▏       | 108/499 [19:08<1:10:41, 10.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|██▏       | 109/499 [19:16<1:05:47, 10.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|██▏       | 110/499 [19:23<59:04,  9.11s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|██▏       | 111/499 [19:31<56:38,  8.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|██▏       | 112/499 [19:58<1:31:16, 14.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|██▎       | 113/499 [20:07<1:21:15, 12.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|██▎       | 114/499 [20:12<1:05:35, 10.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|██▎       | 115/499 [20:18<57:53,  9.05s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|██▎       | 116/499 [20:23<50:49,  7.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|██▎       | 117/499 [20:31<50:19,  7.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██▎       | 118/499 [20:45<1:01:28,  9.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██▍       | 119/499 [21:10<1:31:25, 14.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██▍       | 120/499 [21:19<1:19:38, 12.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██▍       | 121/499 [21:27<1:11:03, 11.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██▍       | 122/499 [21:41<1:17:01, 12.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██▍       | 123/499 [21:48<1:05:09, 10.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██▍       | 124/499 [21:53<55:16,  8.84s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██▌       | 125/499 [22:02<56:40,  9.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██▌       | 126/499 [22:13<58:54,  9.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██▌       | 127/499 [22:30<1:13:44, 11.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|██▌       | 128/499 [22:36<1:01:45,  9.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|██▌       | 129/499 [22:40<50:53,  8.25s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|██▌       | 130/499 [22:44<42:33,  6.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|██▋       | 131/499 [22:54<49:07,  8.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|██▋       | 132/499 [23:04<51:25,  8.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 133/499 [23:30<1:24:26, 13.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 134/499 [23:41<1:17:42, 12.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 135/499 [23:52<1:15:33, 12.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 136/499 [24:03<1:12:01, 11.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 137/499 [24:19<1:19:32, 13.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|██▊       | 138/499 [24:26<1:07:56, 11.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|██▊       | 139/499 [24:32<57:49,  9.64s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|██▊       | 140/499 [24:40<55:36,  9.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|██▊       | 141/499 [24:55<1:05:15, 10.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|██▊       | 142/499 [25:07<1:07:03, 11.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|██▊       | 143/499 [26:04<2:27:40, 24.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|██▉       | 144/499 [26:14<2:01:02, 20.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|██▉       | 145/499 [26:20<1:34:45, 16.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|██▉       | 146/499 [26:27<1:19:01, 13.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|██▉       | 147/499 [26:36<1:11:56, 12.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|██▉       | 148/499 [26:42<59:45, 10.21s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|██▉       | 149/499 [26:58<1:10:20, 12.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|███       | 150/499 [27:06<1:03:20, 10.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|███       | 151/499 [27:13<56:07,  9.68s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|███       | 152/499 [27:21<51:56,  8.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███       | 153/499 [27:26<46:21,  8.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███       | 154/499 [27:36<48:55,  8.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███       | 155/499 [27:43<45:51,  8.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███▏      | 156/499 [28:06<1:12:18, 12.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███▏      | 157/499 [28:22<1:16:36, 13.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|███▏      | 158/499 [28:41<1:27:04, 15.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|███▏      | 159/499 [28:47<1:09:39, 12.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|███▏      | 160/499 [28:52<57:45, 10.22s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|███▏      | 161/499 [29:05<1:01:50, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|███▏      | 162/499 [29:16<1:01:57, 11.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|███▎      | 163/499 [29:22<54:09,  9.67s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|███▎      | 164/499 [29:30<50:36,  9.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|███▎      | 165/499 [29:35<43:58,  7.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|███▎      | 166/499 [29:41<39:32,  7.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|███▎      | 167/499 [29:55<51:46,  9.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▎      | 168/499 [30:11<1:03:17, 11.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▍      | 169/499 [30:17<53:01,  9.64s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▍      | 170/499 [30:37<1:09:30, 12.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▍      | 171/499 [30:45<1:02:32, 11.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▍      | 172/499 [30:57<1:02:42, 11.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 35%|███▍      | 173/499 [31:18<1:18:20, 14.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 35%|███▍      | 174/499 [31:24<1:05:04, 12.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 35%|███▌      | 175/499 [31:31<55:43, 10.32s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 35%|███▌      | 176/499 [31:42<56:19, 10.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 35%|███▌      | 177/499 [31:46<45:55,  8.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|███▌      | 178/499 [31:52<41:34,  7.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|███▌      | 179/499 [31:58<39:43,  7.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|███▌      | 180/499 [32:11<48:34,  9.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|███▋      | 181/499 [32:16<41:50,  7.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|███▋      | 182/499 [32:23<39:11,  7.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███▋      | 183/499 [32:30<39:27,  7.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███▋      | 184/499 [32:49<56:32, 10.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███▋      | 185/499 [33:04<1:02:32, 11.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███▋      | 186/499 [33:09<51:31,  9.88s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███▋      | 187/499 [33:16<47:15,  9.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 188/499 [33:28<52:01, 10.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 189/499 [33:53<1:14:23, 14.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 190/499 [33:59<1:01:01, 11.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 191/499 [34:02<47:51,  9.32s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 192/499 [34:14<51:07,  9.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 39%|███▊      | 193/499 [34:19<44:24,  8.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 39%|███▉      | 194/499 [34:27<42:06,  8.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 39%|███▉      | 195/499 [34:37<45:48,  9.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 39%|███▉      | 196/499 [34:44<42:31,  8.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 39%|███▉      | 197/499 [34:57<48:38,  9.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|███▉      | 198/499 [35:07<49:47,  9.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|███▉      | 199/499 [35:25<1:01:48, 12.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|████      | 200/499 [35:36<58:20, 11.71s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|████      | 201/499 [35:52<1:04:29, 12.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|████      | 202/499 [36:09<1:11:23, 14.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|████      | 203/499 [36:14<56:34, 11.47s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|████      | 204/499 [36:33<1:08:10, 13.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|████      | 205/499 [36:43<1:01:07, 12.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|████▏     | 206/499 [37:03<1:12:04, 14.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|████▏     | 207/499 [37:09<59:12, 12.16s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|████▏     | 208/499 [37:13<47:22,  9.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|████▏     | 209/499 [37:21<45:13,  9.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|████▏     | 210/499 [37:29<43:03,  8.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|████▏     | 211/499 [37:39<44:36,  9.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|████▏     | 212/499 [37:51<47:45,  9.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|████▎     | 213/499 [37:56<40:38,  8.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|████▎     | 214/499 [38:19<1:01:03, 12.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|████▎     | 215/499 [38:29<56:52, 12.02s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|████▎     | 216/499 [38:35<48:04, 10.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|████▎     | 217/499 [38:44<46:16,  9.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|████▎     | 218/499 [38:54<45:42,  9.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|████▍     | 219/499 [39:01<42:03,  9.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|████▍     | 220/499 [39:14<47:47, 10.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|████▍     | 221/499 [39:28<52:50, 11.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|████▍     | 222/499 [39:38<50:25, 10.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|████▍     | 223/499 [39:55<58:17, 12.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|████▍     | 224/499 [40:00<48:02, 10.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|████▌     | 225/499 [40:05<39:30,  8.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|████▌     | 226/499 [40:13<38:30,  8.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|████▌     | 227/499 [40:21<37:35,  8.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|████▌     | 228/499 [40:26<34:02,  7.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|████▌     | 229/499 [40:53<1:00:25, 13.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|████▌     | 230/499 [40:59<49:16, 10.99s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|████▋     | 231/499 [41:05<43:09,  9.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|████▋     | 232/499 [41:24<55:41, 12.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|████▋     | 233/499 [41:38<57:22, 12.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|████▋     | 234/499 [41:45<49:00, 11.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|████▋     | 235/499 [41:54<45:53, 10.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|████▋     | 236/499 [42:09<51:14, 11.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|████▋     | 237/499 [42:26<57:43, 13.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 238/499 [42:38<56:03, 12.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 239/499 [42:54<1:00:38, 13.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 240/499 [43:02<52:05, 12.07s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 241/499 [43:15<53:48, 12.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 242/499 [43:23<47:45, 11.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████▊     | 243/499 [43:32<44:47, 10.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████▉     | 244/499 [43:40<41:42,  9.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████▉     | 245/499 [43:45<34:10,  8.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████▉     | 246/499 [43:53<34:55,  8.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████▉     | 247/499 [44:21<59:07, 14.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|████▉     | 248/499 [44:31<54:02, 12.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|████▉     | 249/499 [44:45<55:33, 13.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|█████     | 250/499 [44:57<53:36, 12.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|█████     | 251/499 [45:03<44:28, 10.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████     | 252/499 [45:09<38:10,  9.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████     | 253/499 [45:28<49:48, 12.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████     | 254/499 [45:43<53:37, 13.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████     | 255/499 [45:50<46:04, 11.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████▏    | 256/499 [46:04<48:54, 12.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▏    | 257/499 [46:21<54:07, 13.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▏    | 258/499 [46:36<56:40, 14.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▏    | 259/499 [46:41<45:03, 11.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▏    | 260/499 [46:52<44:18, 11.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▏    | 261/499 [47:03<44:19, 11.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|█████▎    | 262/499 [47:11<40:20, 10.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|█████▎    | 263/499 [47:27<47:15, 12.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|█████▎    | 264/499 [47:43<51:51, 13.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|█████▎    | 265/499 [47:52<46:04, 11.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|█████▎    | 266/499 [48:00<41:52, 10.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|█████▎    | 267/499 [48:12<42:59, 11.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|█████▎    | 268/499 [48:17<35:45,  9.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|█████▍    | 269/499 [48:40<51:00, 13.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|█████▍    | 270/499 [48:47<43:20, 11.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|█████▍    | 271/499 [48:58<42:34, 11.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▍    | 272/499 [49:09<42:56, 11.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▍    | 273/499 [49:16<38:02, 10.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▍    | 274/499 [49:25<36:22,  9.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▌    | 275/499 [49:38<39:26, 10.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▌    | 276/499 [49:41<31:36,  8.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|█████▌    | 277/499 [49:54<36:01,  9.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|█████▌    | 278/499 [50:04<35:41,  9.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|█████▌    | 279/499 [50:15<37:00, 10.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|█████▌    | 280/499 [50:25<37:07, 10.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|█████▋    | 281/499 [50:30<31:03,  8.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|█████▋    | 282/499 [50:38<31:02,  8.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|█████▋    | 283/499 [50:49<33:27,  9.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|█████▋    | 284/499 [51:00<34:27,  9.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|█████▋    | 285/499 [51:15<40:44, 11.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|█████▋    | 286/499 [51:23<36:49, 10.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|█████▊    | 287/499 [51:27<29:33,  8.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|█████▊    | 288/499 [51:39<33:10,  9.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|█████▊    | 289/499 [51:51<35:26, 10.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|█████▊    | 290/499 [52:05<39:22, 11.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|█████▊    | 291/499 [52:21<44:22, 12.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|█████▊    | 292/499 [52:25<35:32, 10.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|█████▊    | 293/499 [52:33<32:13,  9.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|█████▉    | 294/499 [52:43<33:03,  9.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|█████▉    | 295/499 [53:04<44:43, 13.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|█████▉    | 296/499 [53:12<39:15, 11.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|█████▉    | 297/499 [53:25<40:12, 11.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|█████▉    | 298/499 [53:33<35:48, 10.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|█████▉    | 299/499 [53:48<39:52, 11.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|██████    | 300/499 [53:54<33:44, 10.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|██████    | 301/499 [53:59<28:35,  8.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 61%|██████    | 302/499 [54:04<24:58,  7.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 61%|██████    | 303/499 [54:11<24:41,  7.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 61%|██████    | 304/499 [54:20<25:50,  7.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 61%|██████    | 305/499 [54:25<22:37,  7.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 61%|██████▏   | 306/499 [54:29<19:13,  5.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 307/499 [54:35<19:33,  6.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 308/499 [54:42<20:32,  6.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 309/499 [54:50<21:25,  6.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 310/499 [55:16<39:14, 12.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 311/499 [55:24<34:53, 11.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|██████▎   | 312/499 [55:36<36:00, 11.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|██████▎   | 313/499 [55:44<32:19, 10.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|██████▎   | 314/499 [55:49<26:52,  8.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|██████▎   | 315/499 [56:01<29:48,  9.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|██████▎   | 316/499 [56:12<30:35, 10.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████▎   | 317/499 [56:14<23:55,  7.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████▎   | 318/499 [56:25<26:16,  8.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████▍   | 319/499 [56:42<33:54, 11.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████▍   | 320/499 [56:52<31:45, 10.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████▍   | 321/499 [57:00<29:50, 10.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 65%|██████▍   | 322/499 [57:09<28:20,  9.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 65%|██████▍   | 323/499 [57:18<27:43,  9.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 65%|██████▍   | 324/499 [57:37<35:51, 12.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 65%|██████▌   | 325/499 [57:50<36:33, 12.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 65%|██████▌   | 326/499 [58:00<34:04, 11.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▌   | 327/499 [58:06<28:43, 10.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▌   | 328/499 [58:21<32:44, 11.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▌   | 329/499 [58:38<37:19, 13.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▌   | 330/499 [59:03<46:51, 16.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▋   | 331/499 [59:15<43:17, 15.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|██████▋   | 332/499 [59:23<36:13, 13.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|██████▋   | 333/499 [59:28<29:54, 10.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|██████▋   | 334/499 [59:34<25:32,  9.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|██████▋   | 335/499 [59:46<27:16,  9.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|██████▋   | 336/499 [59:54<25:40,  9.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|██████▊   | 337/499 [1:00:07<28:26, 10.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|██████▊   | 338/499 [1:00:16<27:01, 10.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|██████▊   | 339/499 [1:00:21<22:31,  8.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|██████▊   | 340/499 [1:00:40<30:55, 11.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|██████▊   | 341/499 [1:00:47<26:50, 10.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▊   | 342/499 [1:01:03<31:19, 11.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▊   | 343/499 [1:01:13<29:44, 11.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▉   | 344/499 [1:01:29<33:03, 12.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▉   | 345/499 [1:01:48<37:50, 14.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▉   | 346/499 [1:02:00<35:21, 13.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|██████▉   | 347/499 [1:02:16<37:00, 14.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|██████▉   | 348/499 [1:02:25<32:37, 12.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|██████▉   | 349/499 [1:02:40<33:22, 13.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|███████   | 350/499 [1:02:44<26:27, 10.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|███████   | 351/499 [1:02:54<25:30, 10.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|███████   | 352/499 [1:03:01<23:15,  9.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|███████   | 353/499 [1:03:12<23:53,  9.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|███████   | 354/499 [1:03:18<20:57,  8.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|███████   | 355/499 [1:03:30<23:03,  9.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|███████▏  | 356/499 [1:03:34<19:30,  8.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|███████▏  | 357/499 [1:03:38<16:25,  6.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|███████▏  | 358/499 [1:03:42<13:37,  5.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|███████▏  | 359/499 [1:03:53<17:28,  7.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|███████▏  | 360/499 [1:04:00<16:46,  7.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|███████▏  | 361/499 [1:04:05<15:31,  6.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 362/499 [1:04:09<13:24,  5.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 363/499 [1:04:23<18:38,  8.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 364/499 [1:04:33<19:37,  8.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 365/499 [1:04:39<17:32,  7.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 366/499 [1:04:43<15:14,  6.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|███████▎  | 367/499 [1:04:48<14:04,  6.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|███████▎  | 368/499 [1:04:59<16:26,  7.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|███████▍  | 369/499 [1:05:10<19:03,  8.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|███████▍  | 370/499 [1:05:17<17:45,  8.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|███████▍  | 371/499 [1:05:22<15:03,  7.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|███████▍  | 372/499 [1:05:25<12:27,  5.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|███████▍  | 373/499 [1:05:32<13:13,  6.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|███████▍  | 374/499 [1:05:45<17:26,  8.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|███████▌  | 375/499 [1:05:56<19:01,  9.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|███████▌  | 376/499 [1:06:17<26:06, 12.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████▌  | 377/499 [1:06:26<23:22, 11.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████▌  | 378/499 [1:06:33<20:38, 10.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████▌  | 379/499 [1:06:42<19:38,  9.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████▌  | 380/499 [1:07:17<34:32, 17.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████▋  | 381/499 [1:07:31<31:51, 16.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████▋  | 382/499 [1:07:42<28:31, 14.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████▋  | 383/499 [1:08:12<37:27, 19.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████▋  | 384/499 [1:08:29<35:49, 18.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████▋  | 385/499 [1:08:41<31:50, 16.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████▋  | 386/499 [1:08:48<25:38, 13.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|███████▊  | 387/499 [1:08:57<23:10, 12.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|███████▊  | 388/499 [1:09:08<22:09, 11.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|███████▊  | 389/499 [1:09:15<19:06, 10.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|███████▊  | 390/499 [1:09:25<18:40, 10.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|███████▊  | 391/499 [1:09:34<18:02, 10.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|███████▊  | 392/499 [1:09:47<19:20, 10.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|███████▉  | 393/499 [1:09:57<18:26, 10.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|███████▉  | 394/499 [1:10:04<16:44,  9.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|███████▉  | 395/499 [1:10:15<17:20, 10.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|███████▉  | 396/499 [1:10:35<22:20, 13.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|███████▉  | 397/499 [1:10:52<23:55, 14.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|███████▉  | 398/499 [1:11:10<25:46, 15.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|███████▉  | 399/499 [1:11:24<24:53, 14.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|████████  | 400/499 [1:11:31<20:33, 12.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|████████  | 401/499 [1:11:50<23:30, 14.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|████████  | 402/499 [1:11:58<20:09, 12.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|████████  | 403/499 [1:12:03<16:38, 10.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|████████  | 404/499 [1:12:13<16:25, 10.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|████████  | 405/499 [1:12:19<14:05,  8.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|████████▏ | 406/499 [1:12:23<11:31,  7.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|████████▏ | 407/499 [1:12:33<12:29,  8.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|████████▏ | 408/499 [1:12:46<14:42,  9.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|████████▏ | 409/499 [1:12:56<14:32,  9.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|████████▏ | 410/499 [1:13:06<14:44,  9.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|████████▏ | 411/499 [1:13:28<19:41, 13.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|████████▎ | 412/499 [1:13:36<16:56, 11.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|████████▎ | 413/499 [1:13:45<15:59, 11.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|████████▎ | 414/499 [1:13:58<16:23, 11.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|████████▎ | 415/499 [1:14:19<20:07, 14.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|████████▎ | 416/499 [1:14:26<17:01, 12.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|████████▎ | 417/499 [1:14:31<13:31,  9.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|████████▍ | 418/499 [1:14:37<11:58,  8.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|████████▍ | 419/499 [1:14:42<10:18,  7.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|████████▍ | 420/499 [1:15:11<18:35, 14.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|████████▍ | 421/499 [1:15:17<15:07, 11.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|████████▍ | 422/499 [1:15:29<15:05, 11.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|████████▍ | 423/499 [1:15:40<14:25, 11.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|████████▍ | 424/499 [1:15:47<12:40, 10.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|████████▌ | 425/499 [1:15:56<12:11,  9.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|████████▌ | 426/499 [1:16:05<11:43,  9.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▌ | 427/499 [1:16:12<10:25,  8.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▌ | 428/499 [1:16:23<11:13,  9.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▌ | 429/499 [1:16:31<10:40,  9.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▌ | 430/499 [1:16:37<09:12,  8.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▋ | 431/499 [1:16:43<08:31,  7.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|████████▋ | 432/499 [1:16:50<08:16,  7.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|████████▋ | 433/499 [1:17:02<09:36,  8.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|████████▋ | 434/499 [1:17:12<09:58,  9.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|████████▋ | 435/499 [1:19:04<42:25, 39.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|████████▋ | 436/499 [1:19:16<33:05, 31.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████▊ | 437/499 [1:19:23<25:04, 24.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████▊ | 438/499 [1:19:27<18:18, 18.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████▊ | 439/499 [1:19:39<16:12, 16.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████▊ | 440/499 [1:19:52<15:11, 15.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████▊ | 441/499 [1:20:06<14:22, 14.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████▊ | 442/499 [1:20:16<12:42, 13.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████▉ | 443/499 [1:20:29<12:26, 13.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████▉ | 444/499 [1:20:47<13:40, 14.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████▉ | 445/499 [1:21:02<13:14, 14.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████▉ | 446/499 [1:21:17<13:10, 14.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|████████▉ | 447/499 [1:21:21<10:10, 11.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|████████▉ | 448/499 [1:21:31<09:27, 11.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|████████▉ | 449/499 [1:21:36<07:47,  9.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|█████████ | 450/499 [1:21:52<09:08, 11.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|█████████ | 451/499 [1:22:02<08:47, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|█████████ | 452/499 [1:22:10<07:47,  9.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|█████████ | 453/499 [1:22:14<06:24,  8.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|█████████ | 454/499 [1:22:23<06:21,  8.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|█████████ | 455/499 [1:22:31<05:59,  8.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|█████████▏| 456/499 [1:22:38<05:34,  7.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████▏| 457/499 [1:22:45<05:28,  7.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████▏| 458/499 [1:22:54<05:31,  8.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████▏| 459/499 [1:22:58<04:31,  6.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████▏| 460/499 [1:23:11<05:33,  8.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████▏| 461/499 [1:23:17<05:00,  7.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 462/499 [1:23:23<04:26,  7.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 463/499 [1:23:36<05:22,  8.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 464/499 [1:23:38<04:04,  7.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 465/499 [1:23:50<04:45,  8.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 466/499 [1:23:55<04:07,  7.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|█████████▎| 467/499 [1:24:04<04:13,  7.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|█████████▍| 468/499 [1:24:34<07:28, 14.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|█████████▍| 469/499 [1:24:48<07:13, 14.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|█████████▍| 470/499 [1:24:53<05:31, 11.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|█████████▍| 471/499 [1:25:06<05:34, 11.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|█████████▍| 472/499 [1:25:17<05:18, 11.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|█████████▍| 473/499 [1:25:23<04:17,  9.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|█████████▍| 474/499 [1:25:36<04:34, 10.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|█████████▌| 475/499 [1:25:57<05:36, 14.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|█████████▌| 476/499 [1:26:09<05:06, 13.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|█████████▌| 477/499 [1:26:18<04:25, 12.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|█████████▌| 478/499 [1:26:45<05:47, 16.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|█████████▌| 479/499 [1:27:01<05:29, 16.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|█████████▌| 480/499 [1:27:19<05:18, 16.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|█████████▋| 481/499 [1:27:43<05:39, 18.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|█████████▋| 482/499 [1:27:51<04:29, 15.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|█████████▋| 483/499 [1:28:01<03:43, 13.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|█████████▋| 484/499 [1:28:26<04:20, 17.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|█████████▋| 485/499 [1:28:30<03:07, 13.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|█████████▋| 486/499 [1:28:39<02:34, 11.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|█████████▊| 487/499 [1:28:56<02:41, 13.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|█████████▊| 488/499 [1:29:00<01:56, 10.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|█████████▊| 489/499 [1:29:20<02:13, 13.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|█████████▊| 490/499 [1:29:27<01:44, 11.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|█████████▊| 491/499 [1:29:56<02:13, 16.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|█████████▊| 492/499 [1:30:05<01:41, 14.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|█████████▉| 493/499 [1:30:17<01:22, 13.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|█████████▉| 494/499 [1:30:21<00:54, 10.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|█████████▉| 495/499 [1:30:27<00:37,  9.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|█████████▉| 496/499 [1:30:35<00:26,  8.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|█████████▉| 497/499 [1:31:06<00:31, 15.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|█████████▉| 498/499 [1:31:14<00:13, 13.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|██████████| 499/499 [1:31:31<00:00, 11.01s/it]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "test = DatasetForLlamaTest(r'/data/ephemeral/home/data/test.csv')\n",
    "\n",
    "fname_list = []\n",
    "summary_list = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for fname, dialogue in tqdm(test):\n",
    "        generated = model.generate(**dialogue, max_new_tokens=200)\n",
    "        decoded_str = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "        \n",
    "        summary = decoded_str.split('Summary:\\n')[1]\n",
    "        \n",
    "        # \\n 제거\n",
    "        summary = re.sub('\\n', '', summary)\n",
    "        \n",
    "        fname_list.append(fname)\n",
    "        summary_list.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'fname': fname_list,\n",
    "    'summary': summary_list,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(r'/data/ephemeral/home/upstage-nlp-summarization-nlp11/prediction/Meta-Llama-3.1-8B_epoch1_lora32_drop0.1_maxtoken200.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "generate 파라미터 테스트를 위한 inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "다음 대화를 요약하세요:\n",
      "#Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?\n",
      "#Person2#: 건강검진을 받는 것이 좋을 것 같아서요.\n",
      "#Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 않았습니다. 매년 받아야 합니다.\n",
      "#Person2#: 알고 있습니다. 하지만 아무 문제가 없다면 왜 의사를 만나러 가야 하나요?\n",
      "#Person1#: 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 오세요.\n",
      "#Person2#: 알겠습니다.\n",
      "#Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 피우시나요?\n",
      "#Person2#: 네.\n",
      "#Person1#: 당신도 알다시피, 담배는 폐암과 심장병의 주요 원인입니다. 정말로 끊으셔야 합니다. \n",
      "#Person2#: 수백 번 시도했지만, 습관을 버리는 것이 어렵습니다.\n",
      "#Person1#: 우리는 도움이 될 수 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다.\n",
      "#Person2#: 알겠습니다, 감사합니다, 의사선생님.\n",
      "---\n",
      "요약:\n",
      "스미스의 건강 검진에 대한 의사의 의견은 다음과 같습니다. 첫째, 그는 매일 건강 체크를 받을 필요가 있다고 생각합니다. 두 번째로, 그의 건강 상태는 좋지 않으며, 특히 그가 흡연하는 것에 대해 걱정됩니다. 마지막으로, 그들은 그를 위해 도움을 줄 수 있다고 말했습니다.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "eval_prompt = \"\"\"\n",
    "다음 대화를 요약하세요:\n",
    "#Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?\\n#Person2#: 건강검진을 받는 것이 좋을 것 같아서요.\\n#Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 않았습니다. 매년 받아야 합니다.\\n#Person2#: 알고 있습니다. 하지만 아무 문제가 없다면 왜 의사를 만나러 가야 하나요?\\n#Person1#: 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 오세요.\\n#Person2#: 알겠습니다.\\n#Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 피우시나요?\\n#Person2#: 네.\\n#Person1#: 당신도 알다시피, 담배는 폐암과 심장병의 주요 원인입니다. 정말로 끊으셔야 합니다. \\n#Person2#: 수백 번 시도했지만, 습관을 버리는 것이 어렵습니다.\\n#Person1#: 우리는 도움이 될 수 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다.\\n#Person2#: 알겠습니다, 감사합니다, 의사선생님.\n",
    "---\n",
    "요약:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.inference_mode():\n",
    "    print(tokenizer.decode(model.generate(**model_input, \n",
    "                                        max_new_tokens=100,      \n",
    "                                        num_beams=5,   \n",
    "                                        do_sample=False,                       \n",
    "                                        # temperature=1.5,  # 다양성 제어\n",
    "                                        # top_p=0.9,        # top-p 샘플링 (Nucleus Sampling)\n",
    "                                        repetition_penalty=1.1,  # 반복 방지\n",
    "                                        no_repeat_ngram_size=2\n",
    "                                    )[0], \n",
    "                           skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "다음 대화를 요약하세요:\n",
      "#Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?\n",
      "#Person2#: 건강검진을 받는 것이 좋을 것 같아서요.\n",
      "#Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 않았습니다. 매년 받아야 합니다.\n",
      "#Person2#: 알고 있습니다. 하지만 아무 문제가 없다면 왜 의사를 만나러 가야 하나요?\n",
      "#Person1#: 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 오세요.\n",
      "#Person2#: 알겠습니다.\n",
      "#Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 피우시나요?\n",
      "#Person2#: 네.\n",
      "#Person1#: 당신도 알다시피, 담배는 폐암과 심장병의 주요 원인입니다. 정말로 끊으셔야 합니다. \n",
      "#Person2#: 수백 번 시도했지만, 습관을 버리는 것이 어렵습니다.\n",
      "#Person1#: 우리는 도움이 될 수 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다.\n",
      "#Person2#: 알겠습니다, 감사합니다, 의사선생님.\n",
      "---\n",
      "요약:\n",
      "스미스씨는 건강검진을 위해 의사에게 오고 있습니다. 호킨스 의사는 스미스씨에게 매년 건강검진을 받는 것이 좋다고 말하고, 스미스씨는 담배를 끊는 것이 어렵다고 말합니다.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "eval_prompt = \"\"\"\n",
    "다음 대화를 요약하세요:\n",
    "#Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?\\n#Person2#: 건강검진을 받는 것이 좋을 것 같아서요.\\n#Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 않았습니다. 매년 받아야 합니다.\\n#Person2#: 알고 있습니다. 하지만 아무 문제가 없다면 왜 의사를 만나러 가야 하나요?\\n#Person1#: 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 오세요.\\n#Person2#: 알겠습니다.\\n#Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 피우시나요?\\n#Person2#: 네.\\n#Person1#: 당신도 알다시피, 담배는 폐암과 심장병의 주요 원인입니다. 정말로 끊으셔야 합니다. \\n#Person2#: 수백 번 시도했지만, 습관을 버리는 것이 어렵습니다.\\n#Person1#: 우리는 도움이 될 수 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다.\\n#Person2#: 알겠습니다, 감사합니다, 의사선생님.\n",
    "---\n",
    "요약:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.inference_mode():\n",
    "    print(tokenizer.decode(model.generate(**model_input, \n",
    "                                        max_new_tokens=100\n",
    "                                    )[0], \n",
    "                           skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "다음 대화를 요약하세요:\n",
      "#Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?\n",
      "#Person2#: 건강검진을 받는 것이 좋을 것 같아서요.\n",
      "#Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 않았습니다. 매년 받아야 합니다.\n",
      "#Person2#: 알고 있습니다. 하지만 아무 문제가 없다면 왜 의사를 만나러 가야 하나요?\n",
      "#Person1#: 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 오세요.\n",
      "#Person2#: 알겠습니다.\n",
      "#Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 피우시나요?\n",
      "#Person2#: 네.\n",
      "#Person1#: 당신도 알다시피, 담배는 폐암과 심장병의 주요 원인입니다. 정말로 끊으셔야 합니다. \n",
      "#Person2#: 수백 번 시도했지만, 습관을 버리는 것이 어렵습니다.\n",
      "#Person1#: 우리는 도움이 될 수 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다.\n",
      "#Person2#: 알겠습니다, 감사합니다, 의사선생님.\n",
      "---\n",
      "요약:\n",
      "스미스씨는 건강검진을 위해 의사에게 찾아갑니다. 의사는 스미스씨에게 건강검진의 중요성을 강조하고, 담배를 끊는 데 도움을 줄 수 있는 수업과 약물들을 제공할 것입니다.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "eval_prompt = \"\"\"\n",
    "다음 대화를 요약하세요:\n",
    "#Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?\\n#Person2#: 건강검진을 받는 것이 좋을 것 같아서요.\\n#Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 않았습니다. 매년 받아야 합니다.\\n#Person2#: 알고 있습니다. 하지만 아무 문제가 없다면 왜 의사를 만나러 가야 하나요?\\n#Person1#: 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 오세요.\\n#Person2#: 알겠습니다.\\n#Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 피우시나요?\\n#Person2#: 네.\\n#Person1#: 당신도 알다시피, 담배는 폐암과 심장병의 주요 원인입니다. 정말로 끊으셔야 합니다. \\n#Person2#: 수백 번 시도했지만, 습관을 버리는 것이 어렵습니다.\\n#Person1#: 우리는 도움이 될 수 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다.\\n#Person2#: 알겠습니다, 감사합니다, 의사선생님.\n",
    "---\n",
    "요약:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.inference_mode():\n",
    "    print(tokenizer.decode(model.generate(**model_input, \n",
    "                                        max_new_tokens=200\n",
    "                                    )[0], \n",
    "                           skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "다음 대화를 요약하세요:\n",
      "#Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?\n",
      "#Person2#: 건강검진을 받는 것이 좋을 것 같아서요.\n",
      "#Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 않았습니다. 매년 받아야 합니다.\n",
      "#Person2#: 알고 있습니다. 하지만 아무 문제가 없다면 왜 의사를 만나러 가야 하나요?\n",
      "#Person1#: 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 오세요.\n",
      "#Person2#: 알겠습니다.\n",
      "#Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 피우시나요?\n",
      "#Person2#: 네.\n",
      "#Person1#: 당신도 알다시피, 담배는 폐암과 심장병의 주요 원인입니다. 정말로 끊으셔야 합니다. \n",
      "#Person2#: 수백 번 시도했지만, 습관을 버리는 것이 어렵습니다.\n",
      "#Person1#: 우리는 도움이 될 수 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다.\n",
      "#Person2#: 알겠습니다, 감사합니다, 의사선생님.\n",
      "---\n",
      "요약:\n",
      "스미스는 건강 검진에 대해 의과학자에게 상담합니다. 의사는 스밀스에게 건강에 대한 정보와 담뱃이를 끄는 방법을 알려줍니다.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "eval_prompt = \"\"\"\n",
    "다음 대화를 요약하세요:\n",
    "#Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?\\n#Person2#: 건강검진을 받는 것이 좋을 것 같아서요.\\n#Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 않았습니다. 매년 받아야 합니다.\\n#Person2#: 알고 있습니다. 하지만 아무 문제가 없다면 왜 의사를 만나러 가야 하나요?\\n#Person1#: 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 오세요.\\n#Person2#: 알겠습니다.\\n#Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 피우시나요?\\n#Person2#: 네.\\n#Person1#: 당신도 알다시피, 담배는 폐암과 심장병의 주요 원인입니다. 정말로 끊으셔야 합니다. \\n#Person2#: 수백 번 시도했지만, 습관을 버리는 것이 어렵습니다.\\n#Person1#: 우리는 도움이 될 수 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다.\\n#Person2#: 알겠습니다, 감사합니다, 의사선생님.\n",
    "---\n",
    "요약:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.inference_mode():\n",
    "    print(tokenizer.decode(model.generate(**model_input, \n",
    "                                        max_new_tokens=200,\n",
    "                                        no_repeat_ngram_size=2\n",
    "                                    )[0], \n",
    "                           skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep  9 08:18:16 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  | 00000000:4C:00.0 Off |                  N/A |\n",
      "| 39%   33C    P8              19W / 350W |  19198MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
