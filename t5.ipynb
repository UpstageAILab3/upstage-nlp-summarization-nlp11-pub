{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt_tab')\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained('eenzeenee/t5-base-korean-summarization')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('eenzeenee/t5-base-korean-summarization')\n",
    "\n",
    "# prefix = \"summarize: \"\n",
    "# sample = \"\"\"#Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?\n",
    "# #Person2#: 건강검진을 받는 것이 좋을 것 같아서요.\n",
    "# #Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 않았습니다. 매년 받아야 합니다.\n",
    "# #Person2#: 알고 있습니다. 하지만 아무 문제가 없다면 왜 의사를 만나러 가야 하나요?\n",
    "# #Person1#: 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 오세요.\n",
    "# #Person2#: 알겠습니다.\n",
    "# #Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 피우시나요?\n",
    "# #Person2#: 네.\n",
    "# #Person1#: 당신도 알다시피, 담배는 폐암과 심장병의 주요 원인입니다. 정말로 끊으셔야 합니다. \n",
    "# #Person2#: 수백 번 시도했지만, 습관을 버리는 것이 어렵습니다.\n",
    "# #Person1#: 우리는 도움이 될 수 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다.\n",
    "# #Person2#: 알겠습니다, 감사합니다, 의사선생님.\"\"\"\n",
    "\n",
    "# inputs = [prefix + sample]\n",
    "\n",
    "\n",
    "# inputs = tokenizer(inputs, max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "# output = model.generate(**inputs, num_beams=3, do_sample=True, min_length=10, max_length=64)\n",
    "# decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "# result = nltk.sent_tokenize(decoded_output.strip())[0]\n",
    "\n",
    "# print('RESULT >>', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "from rouge import Rouge\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "from transformers import logging\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# 로거 생성\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 로거 레벨 설정\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# 콘솔 핸들러 생성\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "# 포매터 생성\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# 로거에 핸들러 추가\n",
    "logger.addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time = datetime.fromtimestamp(time.time(), tz=ZoneInfo(\"Asia/Seoul\")).strftime(\"%m%d-%H%M%S\")\n",
    "train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = ['#Address#', \n",
    "                  '#CarNumber#', \n",
    "                  '#CardNumber#',\n",
    "                  '#DateOfBirth#', \n",
    "                  '#Email#', \n",
    "                  '#PassportNumber#', \n",
    "                  '#Person1#', \n",
    "                  '#Person2#', \n",
    "                  '#Person3#', \n",
    "                  '#Person4#', \n",
    "                  '#Person5#', \n",
    "                  '#Person6#', \n",
    "                  '#Person7#',\n",
    "                  '#PhoneNumber#', \n",
    "                  '#SSN#']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 및 모델 로드\n",
    "model_name = 'eenzeenee/t5-base-korean-summarization'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, additional_special_tokens=SPECIAL_TOKENS)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 성능에 대한 평가 지표를 정의합니다. 본 대회에서는 ROUGE 점수를 통해 모델의 성능을 평가합니다.\n",
    "def compute_metrics(tokenizer,pred):\n",
    "    rouge = Rouge()\n",
    "    predictions = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    predictions[predictions == -100] = tokenizer.pad_token_id\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, clean_up_tokenization_spaces=True)\n",
    "    labels = tokenizer.batch_decode(labels, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    # 정확한 평가를 위해 미리 정의된 불필요한 생성토큰들을 제거합니다.\n",
    "    replaced_predictions = decoded_preds.copy()\n",
    "    replaced_labels = labels.copy()\n",
    "    remove_tokens = ['<usr>', f\"{tokenizer.bos_token}\", f\"{tokenizer.eos_token}\", f\"{tokenizer.pad_token}\"]\n",
    "    for token in remove_tokens:\n",
    "        replaced_predictions = [sentence.replace(token,\" \") for sentence in replaced_predictions]\n",
    "        replaced_labels = [sentence.replace(token,\" \") for sentence in replaced_labels]\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {replaced_predictions[0]}\")\n",
    "    print(f\"GOLD: {replaced_labels[0]}\")\n",
    "\n",
    "    # 최종적인 ROUGE 점수를 계산합니다.\n",
    "    results = rouge.get_scores(replaced_predictions, replaced_labels,avg=True)\n",
    "    logger.info(f\"Evaluation results: {results}\")\n",
    "\n",
    "    # ROUGE 점수 중 F-1 score를 통해 평가합니다.\n",
    "    result = {key: value[\"f\"] for key, value in results.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "train_df = pd.read_csv('./data/train_cleaned.csv')\n",
    "dev_df = pd.read_csv('./data/dev.csv')\n",
    "\n",
    "# 데이터셋 전처리 함수\n",
    "def preprocess_function(example):\n",
    "    input_text = f\"summarize: {example['dialogue']}\"\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(example['summary'], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# 데이터셋 전처리\n",
    "train_dataset = train_df.apply(preprocess_function, axis=1)\n",
    "dev_dataset = dev_df.apply(preprocess_function, axis=1)\n",
    "\n",
    "# 학습 인자 설정\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=20,\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=1,\n",
    "    save_strategy='epoch',\n",
    "    save_steps=1,\n",
    "    logging_strategy='epoch',\n",
    "    logging_steps=1,\n",
    "    save_total_limit=5,\n",
    "    load_best_model_at_end=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    # auto_find_batch_size=False,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=4e-05,\n",
    "    lr_scheduler_type='cosine',\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    seed=42,\n",
    "    gradient_accumulation_steps=4,\n",
    "    # generation_max_length=100,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    ")\n",
    "\n",
    "# 데이터 콜레이터 설정\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Validation loss가 더 이상 개선되지 않을 때 학습을 중단시키는 EarlyStopping 기능을 사용합니다.\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.001,\n",
    ")\n",
    "\n",
    "class CUDAMemoryCleanupCallback(TrainerCallback):\n",
    "    def __init__(self, cleanup_interval=100):\n",
    "        self.cleanup_interval = cleanup_interval\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.cleanup_interval == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            # print(f\"Step {state.global_step}: CUDA memory cleaned up.\")\n",
    "            # print(f\"Current GPU memory usage: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "\n",
    "cuda_cleanup_callback = CUDAMemoryCleanupCallback(cleanup_interval=1)\n",
    "\n",
    "# Trainer 설정\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics = lambda pred: compute_metrics(tokenizer, pred),\n",
    "    callbacks=[cuda_cleanup_callback, early_stopping]\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델의 체크포인트 경로\n",
    "checkpoint_path = './results/checkpoint-9333/'\n",
    "print(f\"checkpoint_path: {checkpoint_path}\")\n",
    "\n",
    "# 토크나이저와 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Special token 추가\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': SPECIAL_TOKENS})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# test.csv 파일 읽기\n",
    "test_data = pd.read_csv('./data/test_cleaned.csv')\n",
    "\n",
    "# 예측 결과를 저장할 리스트\n",
    "predictions = []\n",
    "\n",
    "# 입력 텍스트 전처리\n",
    "def preprocess_function(text):\n",
    "    input_text = f\"summarize: {text}\"\n",
    "    return tokenizer(input_text, max_length=512, truncation=True, return_tensors='pt')\n",
    "\n",
    "for dialogue in test_data['dialogue']:\n",
    "    inputs = preprocess_function({dialogue})\n",
    "    \n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        # num_beams=4,\n",
    "        # max_length=50,\n",
    "        # early_stopping=True,\n",
    "        do_sample=False,\n",
    "        repetition_penalty = 1.1\n",
    "    )\n",
    "    \n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=False)\n",
    "    print(summary)\n",
    "    predictions.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확한 평가를 위하여 노이즈에 해당되는 스페셜 토큰을 제거합니다.\n",
    "def remove_default_token(predictions):\n",
    "    remove_tokens = ['<usr>', f\"{tokenizer.bos_token}\", f\"{tokenizer.eos_token}\", f\"{tokenizer.pad_token}\"]\n",
    "    preprocessed_summary = predictions.copy()\n",
    "    for token in remove_tokens:\n",
    "        preprocessed_summary = [sentence.replace(token,\" \") for sentence in preprocessed_summary]\n",
    "\n",
    "    return preprocessed_summary\n",
    "\n",
    "preprocessed_summary = remove_default_token(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prediction(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # 앞뒤 따옴표 제거 (작은따옴표와 큰따옴표 모두 처리)\n",
    "    text = text.strip(\"'\\\"\")\n",
    "\n",
    "    # 문자열의 앞뒤에서 공백 문자(스페이스, 탭, 줄바꿈 등)를 제거\n",
    "    text = text.strip()\n",
    "    \n",
    "    text = text.replace('\\r', '')\n",
    "    text = text.replace('\\n', '')\n",
    "\n",
    "    # 연속된 공백을 단일 공백으로 변경\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "cleaned_sentences = [clean_prediction(sentence) for sentence in preprocessed_summary]\n",
    "cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['summary'] = cleaned_sentences\n",
    "test_data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 파일로 저장\n",
    "test_data = test_data.drop('dialogue', axis=1)\n",
    "test_data.to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "print(\"예측이 완료되었습니다. 결과가 'test_predictions.csv' 파일로 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
